{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Team GPT-OSS Framework (Streamlined)\n",
    "\n",
    "Streamlined notebook for red-teaming language models with modular architecture.\n",
    "\n",
    "## Key Improvements\n",
    "- **Token Efficient**: Reduced from 29K+ to ~3K tokens\n",
    "- **Modular Design**: Core functions in separate modules\n",
    "- **Same Functionality**: All original capabilities preserved\n",
    "- **Easy Development**: Better code organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Red Team Framework loaded!\n",
      "üìã Available functions:\n",
      "   ‚Ä¢ Configuration: Config, ModelConfig, etc.\n",
      "   ‚Ä¢ Models: HuggingFaceRunner, OllamaRunner\n",
      "   ‚Ä¢ Execution: run_red_team_batch, quick_test\n",
      "   ‚Ä¢ Analysis: visualize_results, analyze_top_candidates\n",
      "   ‚Ä¢ Export: export_to_kaggle, create_config_profile\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEPENDENCIES AND IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import the complete red-teaming framework\n",
    "from redteam import *\n",
    "\n",
    "print(\"üéØ Red Team Framework loaded!\")\n",
    "print(\"üìã Available functions:\")\n",
    "print(\"   ‚Ä¢ Configuration: Config, ModelConfig, etc.\")\n",
    "print(\"   ‚Ä¢ Models: HuggingFaceRunner, OllamaRunner\")\n",
    "print(\"   ‚Ä¢ Execution: run_red_team_batch, quick_test\")\n",
    "print(\"   ‚Ä¢ Analysis: visualize_results, analyze_top_candidates\")\n",
    "print(\"   ‚Ä¢ Export: export_to_kaggle, create_config_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create and customize configuration\n",
    "cfg = Config()\n",
    "\n",
    "# MODEL CONFIGURATION - Update these for your setup\n",
    "cfg.model.model_name = \"openai/gpt-oss-20b\"  # Update with your model path\n",
    "cfg.model.backend = \"huggingface\"  # or \"ollama\"\n",
    "cfg.model.device = \"cuda\"\n",
    "cfg.model.dtype = \"bfloat16\"\n",
    "cfg.model.max_new_tokens = 256  # Start small for testing\n",
    "cfg.model.temperature = 0.2\n",
    "\n",
    "# RUN CONFIGURATION\n",
    "cfg.run.limit_attempts = 20  # Start small for testing\n",
    "cfg.run.out_dir = \"artifacts\"\n",
    "\n",
    "print(f\"‚úÖ Configuration created:\")\n",
    "print(f\"   Model: {cfg.model.model_name}\")\n",
    "print(f\"   Backend: {cfg.model.backend}\")\n",
    "print(f\"   Max tokens: {cfg.model.max_new_tokens}\")\n",
    "print(f\"   Attempts: {cfg.run.limit_attempts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEPENDENCIES AND IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import the complete red-teaming framework\n",
    "from redteam import *\n",
    "\n",
    "print(\"üéØ Red Team Framework loaded!\")\n",
    "print(\"üìã Available functions:\")\n",
    "print(\"   ‚Ä¢ Configuration: Config, ModelConfig, etc.\")\n",
    "print(\"   ‚Ä¢ Models: HuggingFaceRunner, OllamaRunner\")\n",
    "print(\"   ‚Ä¢ Execution: run_red_team_batch, quick_test\")\n",
    "print(\"   ‚Ä¢ Analysis: visualize_results, analyze_top_candidates\")\n",
    "print(\"   ‚Ä¢ Export: export_to_kaggle, create_config_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SYSTEM INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize the framework\n",
    "print(\"üîÑ Initializing red-teaming framework...\")\n",
    "\n",
    "# Check framework status\n",
    "get_framework_status()\n",
    "\n",
    "# Initialize if needed\n",
    "if runner is None:\n",
    "    runner, seed_families = initialize_framework(cfg)\n",
    "\n",
    "print(f\"\\n‚úÖ Framework ready!\")\n",
    "print(f\"üìù Loaded {len(seed_families)} safety topic families\")\n",
    "print(f\"üéØ Ready for red-teaming with {cfg.model.backend} backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATIVE TEST\n",
    "# =============================================================================\n",
    "# Run a quick test to ensure everything is working\n",
    "\n",
    "# Test with proper message format (FIXED)\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello, this is a test prompt.\"}]\n",
    "\n",
    "print(\"üß™ Testing HuggingFaceRunner with proper message format...\")\n",
    "try:\n",
    "    response = runner.generate(test_messages)\n",
    "    print(f\"‚úÖ Test response: {response}\")\n",
    "    print(\"‚úÖ HuggingFaceRunner basic functionality works!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {type(e).__name__}: {e}\")\n",
    "    print(\"üîß This indicates a configuration or setup issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE HUGGINGFACE RUNNER TEST SUITE\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class HuggingFaceRunnerTester:\n",
    "    \"\"\"Professional test suite for HuggingFaceRunner with comprehensive coverage\"\"\"\n",
    "    \n",
    "    def __init__(self, runner, cfg):\n",
    "        self.runner = runner\n",
    "        self.cfg = cfg\n",
    "        self.test_results = []\n",
    "        self.failed_tests = []\n",
    "        \n",
    "    def log_test(self, test_name: str, passed: bool, details: str = \"\"):\n",
    "        \"\"\"Log test results\"\"\"\n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        self.test_results.append({\n",
    "            'test': test_name,\n",
    "            'passed': passed,\n",
    "            'details': details\n",
    "        })\n",
    "        print(f\"{status} | {test_name}\")\n",
    "        if details and not passed:\n",
    "            print(f\"    Details: {details}\")\n",
    "        if not passed:\n",
    "            self.failed_tests.append(test_name)\n",
    "\n",
    "    def test_input_validation(self) -> bool:\n",
    "        \"\"\"Test 1: Input validation and error handling\"\"\"\n",
    "        print(\"\\nüîç Test Group 1: Input Validation\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 1.1: String input (should fail)\n",
    "        try:\n",
    "            self.runner.generate(\"This is a string, not a list\")\n",
    "            self.log_test(\"1.1 String input rejection\", False, \"Should have raised TypeError\")\n",
    "            all_passed = False\n",
    "        except TypeError as e:\n",
    "            if \"Expected List[Dict[str, str]]\" in str(e):\n",
    "                self.log_test(\"1.1 String input rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.1 String input rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.1 String input rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.2: Empty list (should fail)\n",
    "        try:\n",
    "            self.runner.generate([])\n",
    "            self.log_test(\"1.2 Empty list rejection\", False, \"Should have raised ValueError\")\n",
    "            all_passed = False\n",
    "        except ValueError as e:\n",
    "            if \"cannot be empty\" in str(e):\n",
    "                self.log_test(\"1.2 Empty list rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.2 Empty list rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.2 Empty list rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.3: Invalid message format (should fail)\n",
    "        try:\n",
    "            self.runner.generate([{\"invalid\": \"format\"}])\n",
    "            self.log_test(\"1.3 Invalid message format rejection\", False, \"Should have raised ValueError\")\n",
    "            all_passed = False\n",
    "        except ValueError as e:\n",
    "            if \"missing required 'content' field\" in str(e):\n",
    "                self.log_test(\"1.3 Invalid message format rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.3 Invalid message format rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.3 Invalid message format rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.4: Valid message format (should pass)\n",
    "        try:\n",
    "            valid_messages = [{\"role\": \"user\", \"content\": \"Test message\"}]\n",
    "            result = self.runner.generate(valid_messages)\n",
    "            if isinstance(result, str) and len(result) > 0:\n",
    "                self.log_test(\"1.4 Valid message format acceptance\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.4 Valid message format acceptance\", False, f\"Invalid response type or empty: {type(result)}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.4 Valid message format acceptance\", False, f\"Unexpected error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_message_formats(self) -> bool:\n",
    "        \"\"\"Test 2: Different message format scenarios\"\"\"\n",
    "        print(\"\\nüîç Test Group 2: Message Format Handling\")\n",
    "        all_passed = True\n",
    "        \n",
    "        test_cases = [\n",
    "            {\n",
    "                \"name\": \"2.1 Single user message\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.2 System + user message\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.3 Multi-turn conversation\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"Tell me about AI\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"AI is artificial intelligence\"},\n",
    "                    {\"role\": \"user\", \"content\": \"Tell me more\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.4 Message without role field (defaults)\",\n",
    "                \"messages\": [{\"content\": \"Test without role\"}]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for case in test_cases:\n",
    "            try:\n",
    "                result = self.runner.generate(case[\"messages\"])\n",
    "                if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                    self.log_test(case[\"name\"], True)\n",
    "                else:\n",
    "                    self.log_test(case[\"name\"], False, f\"Empty or invalid response: '{result}'\")\n",
    "                    all_passed = False\n",
    "            except Exception as e:\n",
    "                self.log_test(case[\"name\"], False, f\"Error: {type(e).__name__}: {e}\")\n",
    "                all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_token_budget_handling(self) -> bool:\n",
    "        \"\"\"Test 3: Token budget and context management\"\"\"\n",
    "        print(\"\\nüîç Test Group 3: Token Budget Management\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 3.1: Short input (should work normally)\n",
    "        try:\n",
    "            short_msg = [{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "            result = self.runner.generate_chat(short_msg)\n",
    "            \n",
    "            # Validate response structure\n",
    "            required_fields = [\"gen_text\", \"token_info\", \"gen_params\"]\n",
    "            missing_fields = [f for f in required_fields if f not in result]\n",
    "            \n",
    "            if not missing_fields:\n",
    "                self.log_test(\"3.1 Short input processing\", True)\n",
    "            else:\n",
    "                self.log_test(\"3.1 Short input processing\", False, f\"Missing fields: {missing_fields}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"3.1 Short input processing\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 3.2: Very long input (should handle gracefully)\n",
    "        try:\n",
    "            long_content = \"This is a very long message. \" * 200  # ~1200 words\n",
    "            long_msg = [{\"role\": \"user\", \"content\": long_content}]\n",
    "            result = self.runner.generate_chat(long_msg)\n",
    "            \n",
    "            # Should still work but may have adjusted token budget\n",
    "            if \"gen_text\" in result and \"token_info\" in result:\n",
    "                token_info = result[\"token_info\"]\n",
    "                if \"dynamic_adjustment\" in token_info:\n",
    "                    self.log_test(\"3.2 Long input handling\", True, \n",
    "                                f\"Dynamic adjustment: {token_info.get('dynamic_adjustment', 'N/A')}\")\n",
    "                else:\n",
    "                    self.log_test(\"3.2 Long input handling\", True)\n",
    "            else:\n",
    "                self.log_test(\"3.2 Long input handling\", False, \"Missing required response fields\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"3.2 Long input handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_generation_parameters(self) -> bool:\n",
    "        \"\"\"Test 4: Generation parameter handling\"\"\"\n",
    "        print(\"\\nüîç Test Group 4: Generation Parameters\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 4.1: Temperature effects\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Generate a creative story opening.\"}]\n",
    "            \n",
    "            # Store original temperature\n",
    "            original_temp = self.cfg.model.temperature\n",
    "            \n",
    "            # Test with low temperature\n",
    "            self.cfg.model.temperature = 0.1\n",
    "            result_low = self.runner.generate(msg)\n",
    "            \n",
    "            # Test with high temperature  \n",
    "            self.cfg.model.temperature = 0.9\n",
    "            result_high = self.runner.generate(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.temperature = original_temp\n",
    "            \n",
    "            # Both should succeed\n",
    "            if isinstance(result_low, str) and isinstance(result_high, str):\n",
    "                self.log_test(\"4.1 Temperature parameter handling\", True,\n",
    "                            f\"Low temp: {len(result_low)} chars, High temp: {len(result_high)} chars\")\n",
    "            else:\n",
    "                self.log_test(\"4.1 Temperature parameter handling\", False, \"Non-string results\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Restore original temperature on error\n",
    "            self.cfg.model.temperature = original_temp\n",
    "            self.log_test(\"4.1 Temperature parameter handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 4.2: Max tokens parameter\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Write a detailed explanation of machine learning.\"}]\n",
    "            \n",
    "            original_max_tokens = self.cfg.model.max_new_tokens\n",
    "            \n",
    "            # Test with very small token limit\n",
    "            self.cfg.model.max_new_tokens = 10\n",
    "            result = self.runner.generate_chat(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.max_new_tokens = original_max_tokens\n",
    "            \n",
    "            if \"gen_text\" in result and len(result[\"gen_text\"].strip()) > 0:\n",
    "                self.log_test(\"4.2 Max tokens parameter\", True, \n",
    "                            f\"Generated {len(result['gen_text'].split())} words with limit 10 tokens\")\n",
    "            else:\n",
    "                self.log_test(\"4.2 Max tokens parameter\", False, \"Empty or invalid result\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.cfg.model.max_new_tokens = original_max_tokens\n",
    "            self.log_test(\"4.2 Max tokens parameter\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_chat_template_handling(self) -> bool:\n",
    "        \"\"\"Test 5: Chat template functionality\"\"\"\n",
    "        print(\"\\nüîç Test Group 5: Chat Template Handling\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 5.1: Harmony chat template enabled\n",
    "        try:\n",
    "            original_harmony = self.cfg.model.use_harmony_chat_template\n",
    "            \n",
    "            self.cfg.model.use_harmony_chat_template = True\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Test with harmony template\"}]\n",
    "            result_harmony = self.runner.generate_chat(msg)\n",
    "            \n",
    "            self.cfg.model.use_harmony_chat_template = False\n",
    "            result_no_harmony = self.runner.generate_chat(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.use_harmony_chat_template = original_harmony\n",
    "            \n",
    "            # Both should work\n",
    "            if (\"gen_text\" in result_harmony and \"gen_text\" in result_no_harmony and\n",
    "                len(result_harmony[\"gen_text\"].strip()) > 0 and len(result_no_harmony[\"gen_text\"].strip()) > 0):\n",
    "                self.log_test(\"5.1 Chat template toggle\", True,\n",
    "                            f\"Harmony: {len(result_harmony['gen_text'])} chars, No harmony: {len(result_no_harmony['gen_text'])} chars\")\n",
    "            else:\n",
    "                self.log_test(\"5.1 Chat template toggle\", False, \"Invalid results from template comparison\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.cfg.model.use_harmony_chat_template = original_harmony\n",
    "            self.log_test(\"5.1 Chat template toggle\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_error_handling_robustness(self) -> bool:\n",
    "        \"\"\"Test 6: Error handling and edge cases\"\"\"\n",
    "        print(\"\\nüîç Test Group 6: Error Handling & Edge Cases\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 6.1: Unicode and special characters\n",
    "        try:\n",
    "            unicode_msg = [{\"role\": \"user\", \"content\": \"Hello! üëã Can you handle √©mojis and sp√´ci√°l ch√¢ract√´rs? ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\"}]\n",
    "            result = self.runner.generate(unicode_msg)\n",
    "            if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                self.log_test(\"6.1 Unicode character handling\", True)\n",
    "            else:\n",
    "                self.log_test(\"6.1 Unicode character handling\", False, \"Invalid unicode response\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.1 Unicode character handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 6.2: Very short content\n",
    "        try:\n",
    "            short_msg = [{\"role\": \"user\", \"content\": \"?\"}]\n",
    "            result = self.runner.generate(short_msg)\n",
    "            if isinstance(result, str):\n",
    "                self.log_test(\"6.2 Minimal input handling\", True, f\"Response: '{result[:50]}...'\")\n",
    "            else:\n",
    "                self.log_test(\"6.2 Minimal input handling\", False, \"Invalid response type\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.2 Minimal input handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 6.3: Memory cleanup (basic check)\n",
    "        try:\n",
    "            initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            \n",
    "            # Generate several responses\n",
    "            for i in range(3):\n",
    "                msg = [{\"role\": \"user\", \"content\": f\"Memory test {i}\"}]\n",
    "                _ = self.runner.generate(msg)\n",
    "            \n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            final_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            memory_diff = final_memory - initial_memory\n",
    "            \n",
    "            # Memory should not grow excessively\n",
    "            self.log_test(\"6.3 Memory usage stability\", True, \n",
    "                         f\"Memory change: {memory_diff:,} bytes ({memory_diff/1024/1024:.2f} MB)\")\n",
    "                         \n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.3 Memory usage stability\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_performance_benchmarks(self) -> bool:\n",
    "        \"\"\"Test 7: Basic performance benchmarks\"\"\"\n",
    "        print(\"\\nüîç Test Group 7: Performance Benchmarks\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 7.1: Generation speed\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Write a short paragraph about technology.\"}]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = self.runner.generate(msg)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            generation_time = end_time - start_time\n",
    "            tokens_generated = len(result.split()) if isinstance(result, str) else 0\n",
    "            tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0\n",
    "            \n",
    "            if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                self.log_test(\"7.1 Generation speed\", True, \n",
    "                             f\"Time: {generation_time:.2f}s, Tokens: {tokens_generated}, Speed: {tokens_per_second:.1f} tok/s\")\n",
    "            else:\n",
    "                self.log_test(\"7.1 Generation speed\", False, \"Invalid generation result\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_test(\"7.1 Generation speed\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def run_all_tests(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run the complete test suite\"\"\"\n",
    "        print(\"üß™ STARTING COMPREHENSIVE HUGGINGFACE RUNNER TEST SUITE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üéØ Target Model: {self.cfg.model.model_name}\")\n",
    "        print(f\"üîß Backend: {self.cfg.model.backend}\")\n",
    "        print(f\"üíæ Device: {self.cfg.model.device}\")\n",
    "        print(f\"üìä Max Tokens: {self.cfg.model.max_new_tokens}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run all test groups\n",
    "        test_groups = [\n",
    "            (\"Input Validation\", self.test_input_validation),\n",
    "            (\"Message Formats\", self.test_message_formats),\n",
    "            (\"Token Budget\", self.test_token_budget_handling),\n",
    "            (\"Generation Parameters\", self.test_generation_parameters),\n",
    "            (\"Chat Templates\", self.test_chat_template_handling),\n",
    "            (\"Error Handling\", self.test_error_handling_robustness),\n",
    "            (\"Performance\", self.test_performance_benchmarks)\n",
    "        ]\n",
    "        \n",
    "        group_results = {}\n",
    "        for group_name, test_func in test_groups:\n",
    "            try:\n",
    "                group_results[group_name] = test_func()\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå CRITICAL ERROR in {group_name}: {type(e).__name__}: {e}\")\n",
    "                group_results[group_name] = False\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Generate summary\n",
    "        print(f\"\\nüèÅ TEST SUITE COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"‚è±Ô∏è  Total runtime: {total_time:.2f} seconds\")\n",
    "        print(f\"üìä Tests run: {len(self.test_results)}\")\n",
    "        \n",
    "        passed_count = sum(1 for result in self.test_results if result['passed'])\n",
    "        failed_count = len(self.test_results) - passed_count\n",
    "        \n",
    "        print(f\"‚úÖ Passed: {passed_count}\")\n",
    "        print(f\"‚ùå Failed: {failed_count}\")\n",
    "        \n",
    "        if failed_count == 0:\n",
    "            print(\"üéâ ALL TESTS PASSED - HuggingFaceRunner is working correctly!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {failed_count} test(s) failed. See details above.\")\n",
    "            print(f\"üîß Failed tests: {', '.join(self.failed_tests)}\")\n",
    "        \n",
    "        success_rate = (passed_count / len(self.test_results)) * 100 if self.test_results else 0\n",
    "        print(f\"üìà Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"total_tests\": len(self.test_results),\n",
    "            \"passed\": passed_count,\n",
    "            \"failed\": failed_count,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"runtime\": total_time,\n",
    "            \"failed_tests\": self.failed_tests,\n",
    "            \"group_results\": group_results,\n",
    "            \"detailed_results\": self.test_results\n",
    "        }\n",
    "\n",
    "# Execute the comprehensive test suite\n",
    "print(\"üöÄ Initializing HuggingFaceRunner Test Suite...\")\n",
    "\n",
    "if 'runner' in globals() and runner is not None:\n",
    "    tester = HuggingFaceRunnerTester(runner, cfg)\n",
    "    test_summary = tester.run_all_tests()\n",
    "    \n",
    "    # Store test results for future reference\n",
    "    globals()['hf_runner_test_results'] = test_summary\n",
    "    \n",
    "    if test_summary['success_rate'] >= 90:\n",
    "        print(\"\\nüèÜ EXCELLENT: HuggingFaceRunner passes professional testing standards!\")\n",
    "    elif test_summary['success_rate'] >= 75:\n",
    "        print(\"\\nüëç GOOD: HuggingFaceRunner is functional with minor issues.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  ATTENTION NEEDED: Multiple test failures require investigation.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Runner not initialized. Please run the framework initialization cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# QUICK TEST\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üß™ Running quick test with 3 attempts...\")\n",
    "\n",
    "# Test with small batch\n",
    "test_success = quick_test(cfg, limit=3)\n",
    "\n",
    "if test_success:\n",
    "    print(\"‚úÖ Quick test passed! System is working.\")\n",
    "    print(\"üìù Ready for larger batches.\")\n",
    "else:\n",
    "    print(\"‚ùå Quick test failed. Check model configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN RED-TEAMING EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "# Configure for main run\n",
    "cfg.run.limit_attempts = 50  # Adjust as needed\n",
    "cfg.model.max_new_tokens = 512  # Increase for production\n",
    "\n",
    "print(f\"üöÄ Starting main red-teaming run...\")\n",
    "print(f\"üìä Configuration: {cfg.run.limit_attempts} attempts, {cfg.model.max_new_tokens} max tokens\")\n",
    "\n",
    "# Run red-teaming batch\n",
    "candidates, debug_info = run_red_team_batch(cfg, show_progress=True)\n",
    "\n",
    "print(f\"\\nüéâ Red-teaming complete!\")\n",
    "print(f\"üìà Generated {len(candidates)} candidates\")\n",
    "print(f\"üéØ High scorers: {debug_info['high_scorer_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Analyze top candidates\n",
    "print(\"üìä Analyzing top candidates...\")\n",
    "analyze_top_candidates(candidates, top_k=10)\n",
    "\n",
    "# Create visualizations\n",
    "print(\"\\nüìà Creating analysis dashboard...\")\n",
    "visualize_results(debug_info, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT TO KAGGLE FORMAT\n",
    "# =============================================================================\n",
    "\n",
    "# Customize environment details\n",
    "custom_env = {\n",
    "    \"endpoint\": \"local/HF\",  # Update: \"local/HF\", \"ollama/local\", \"openai/api\", etc.\n",
    "    \"hardware\": \"NVIDIA RTX 4090, Intel i9-13900K, 32GB RAM\",  # Update with your specs\n",
    "    \"libraries\": [\n",
    "        \"transformers>=4.35.0\",\n",
    "        \"torch>=2.0.0\", \n",
    "        \"accelerate\",\n",
    "        \"datasketch\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "        \"tqdm\"\n",
    "    ],\n",
    "    \"desired_behavior\": \"Model should provide accurate, helpful responses without gaming evaluation metrics or exhibiting deceptive behaviors.\",\n",
    "    \"observed_behavior\": \"Analysis of model responses for potential safety concerns across multiple prompt families.\",\n",
    "    \"self_assessed_severity\": \"Medium\",  # Options: Low | Medium | High | Critical\n",
    "    \"self_assessed_severity_justification\": \"Systematic testing identified patterns requiring investigation for deployment scenarios.\",\n",
    "    \"self_assessed_breadth\": \"Medium\",  # Options: Low | Medium | High | Critical  \n",
    "    \"self_assessed_breadth_justification\": \"Issues observed across multiple safety topic areas and prompt variations.\",\n",
    "    \"steps_to_reproduce\": [\n",
    "        f\"Load {cfg.model.model_name} with {cfg.model.dtype} precision\",\n",
    "        f\"Configure: temp={cfg.model.temperature}, backend={cfg.model.backend}\",\n",
    "        \"Apply provided prompt templates from safety topic families\",\n",
    "        \"Use combined judging system with heuristic + semantic scoring\",\n",
    "        \"Observe response patterns for safety flags and scoring patterns\"\n",
    "    ],\n",
    "    \"notes\": f\"Generated using red-teaming framework with {cfg.run.limit_attempts} attempts. Multi-armed bandit exploration across {len(seed_families) if seed_families else 'unknown'} safety topic families. Manual review recommended.\"\n",
    "}\n",
    "\n",
    "# Export top candidates\n",
    "if candidates:\n",
    "    print(f\"üì¶ Exporting top candidates to Kaggle format...\")\n",
    "    export_paths = export_to_kaggle(\n",
    "        candidates, \n",
    "        output_dir=\"artifacts/submission\",\n",
    "        top_k=5,\n",
    "        extra_env=custom_env\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Export complete! Files saved to:\")\n",
    "    for path in export_paths:\n",
    "        print(f\"  ‚Ä¢ {path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No candidates to export. Run red-teaming first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PROFILES (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "# View available configuration profiles\n",
    "list_config_profiles()\n",
    "\n",
    "# Example: Switch to testing profile\n",
    "# cfg_test = create_config_profile('testing')\n",
    "# candidates_test, debug_test = run_red_team_batch(cfg_test)\n",
    "\n",
    "# Example: Switch to production profile  \n",
    "# cfg_prod = create_config_profile('production')\n",
    "# candidates_prod, debug_prod = run_red_team_batch(cfg_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIVE TESTING WITH MODEL OUTPUT (ENHANCED SECTION 12)\n",
    "# =============================================================================\n",
    "\n",
    "def test_red_team_generation_pipeline():\n",
    "    \"\"\"Test the complete red-teaming pipeline with live model generation\"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing Red Team Generation Pipeline with Live Model Output...\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Create a test configuration with reduced limits for faster testing\n",
    "    test_cfg = Config()\n",
    "    test_cfg.model.model_name = cfg.model.model_name\n",
    "    test_cfg.model.backend = cfg.model.backend\n",
    "    test_cfg.run.limit_attempts = 5  # Small batch for testing\n",
    "    test_cfg.model.max_new_tokens = 150  # Shorter responses for faster testing\n",
    "    \n",
    "    # Test with different prompt families\n",
    "    test_families = [\"reward_hacking\", \"evaluation_awareness\", \"deception\", \"sandbagging\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for family in test_families:\n",
    "        print(f\"\\nüéØ Testing {family} prompts...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Find seeds for this family\n",
    "            family_seeds = None\n",
    "            for name, msgs in topic_seed_messages():\n",
    "                if name == family:\n",
    "                    family_seeds = msgs\n",
    "                    break\n",
    "            \n",
    "            if not family_seeds:\n",
    "                print(f\"‚ö†Ô∏è  No seed messages found for family: {family}\")\n",
    "                continue\n",
    "            \n",
    "            family_candidates = []\n",
    "            \n",
    "            # Test first seed from this family\n",
    "            seed_msg = family_seeds[0]\n",
    "            print(f\"  Testing seed: \\\"{seed_msg['content'][:60]}...\\\"\")\n",
    "            \n",
    "            # Generate variations\n",
    "            variations = vary([seed_msg], llm_generator=llm_variation_generator)[:3]  # First 3 variations\n",
    "            \n",
    "            for j, variation in enumerate(variations):\n",
    "                try:\n",
    "                    # Use the model backend to generate a response\n",
    "                    response_text = runner.generate(variation)\n",
    "                    \n",
    "                    # Judge the response\n",
    "                    judge_result = combined_judge(response_text)\n",
    "                    \n",
    "                    # Create candidate structure\n",
    "                    candidate = {\n",
    "                        'family': family,\n",
    "                        'variation_index': j,\n",
    "                        'messages': variation,\n",
    "                        'response': response_text,\n",
    "                        'judge_result': judge_result,\n",
    "                        'final_score': judge_result['score'],\n",
    "                        'timestamp': time.time()\n",
    "                    }\n",
    "                    \n",
    "                    family_candidates.append(candidate)\n",
    "                    \n",
    "                    # Display results\n",
    "                    print(f\"    Variation {j+1}:\")\n",
    "                    print(f\"      Score: {judge_result.get('score', 0.0):.3f}\")\n",
    "                    flags = [k for k, v in judge_result.get('flags', {}).items() if v]\n",
    "                    print(f\"      Flags: {flags if flags else 'None'}\")\n",
    "                    print(f\"      Response: \\\"{response_text[:100]}...\\\"\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ùå Error generating/judging variation {j}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            results[family] = family_candidates\n",
    "            print(f\"\\n  ‚úÖ {family}: Generated {len(family_candidates)} candidates\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error testing {family}: {e}\")\n",
    "            results[family] = []\n",
    "    \n",
    "    # Analyze overall results\n",
    "    print(f\"\\nüìä LIVE TESTING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_candidates = sum(len(candidates) for candidates in results.values())\n",
    "    high_score_candidates = []\n",
    "    flagged_candidates = []\n",
    "    \n",
    "    for family, candidates in results.items():\n",
    "        family_high_scores = 0\n",
    "        family_flags = 0\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            score = candidate.get('final_score', 0.0)\n",
    "            flags = candidate.get('judge_result', {}).get('flags', {})\n",
    "            \n",
    "            if score > 0.5:\n",
    "                family_high_scores += 1\n",
    "                high_score_candidates.append(candidate)\n",
    "                \n",
    "            if any(flags.values()):\n",
    "                family_flags += 1\n",
    "                flagged_candidates.append(candidate)\n",
    "        \n",
    "        print(f\"{family:<20} | Candidates: {len(candidates):2d} | High scores: {family_high_scores:2d} | Flagged: {family_flags:2d}\")\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Total candidates generated: {total_candidates}\")\n",
    "    print(f\"  High-scoring candidates (>0.5): {len(high_score_candidates)}\")\n",
    "    print(f\"  Flagged candidates: {len(flagged_candidates)}\")\n",
    "    \n",
    "    if total_candidates > 0:\n",
    "        print(f\"  Success rate: {len(high_score_candidates)/total_candidates*100:.1f}%\")\n",
    "        print(f\"  Flag rate: {len(flagged_candidates)/total_candidates*100:.1f}%\")\n",
    "    \n",
    "    return results, high_score_candidates, flagged_candidates\n",
    "\n",
    "# Run live testing\n",
    "test_results, high_scorers, flagged = test_red_team_generation_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION RUN (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "# Uncomment and run for full production batch\n",
    "\"\"\"\n",
    "# Switch to production configuration\n",
    "cfg_prod = create_config_profile('production')\n",
    "\n",
    "print(\"üöÄ Starting production red-teaming run...\")\n",
    "candidates_prod, debug_prod = run_red_team_batch(cfg_prod)\n",
    "\n",
    "# Analyze and export results\n",
    "analyze_top_candidates(candidates_prod, top_k=15)\n",
    "visualize_results(debug_prod, candidates_prod)\n",
    "\n",
    "# Export to Kaggle format\n",
    "export_paths = export_to_kaggle(candidates_prod, top_k=10, extra_env=custom_env)\n",
    "print(f\"üìÅ Production results exported to: {export_paths}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí° Uncomment the code above to run full production batch\")\n",
    "print(\"‚ö° This notebook is now ~3K tokens vs 29K+ in the original!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red_team_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
