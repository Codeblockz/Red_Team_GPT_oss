{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "l2ftgmjs11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Clearing GPU memory...\n",
      "üìä GPU Memory Status:\n",
      "   Total: 31.36 GB\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved: 0.00 GB\n",
      "   Free: 31.36 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MEMORY OPTIMIZATION AND TROUBLESHOOTING\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory\n",
    "print(\"üßπ Clearing GPU memory...\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check current memory usage\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    free = total_memory - allocated\n",
    "    \n",
    "    print(f\"üìä GPU Memory Status:\")\n",
    "    print(f\"   Total: {total_memory:.2f} GB\")\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB\") \n",
    "    print(f\"   Free: {free:.2f} GB\")\n",
    "    \n",
    "    if allocated > 25:  # More than 25GB in use\n",
    "        print(\"‚ö†Ô∏è  High memory usage detected!\")\n",
    "        print(\"üîß Suggestions:\")\n",
    "        print(\"   1. Restart kernel to clear all memory\")\n",
    "        print(\"   2. Use CPU offloading\")\n",
    "        print(\"   3. Use 8-bit or 4-bit quantization\")\n",
    "        print(\"   4. Use smaller model variants\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecebf538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Red Team Framework loaded!\n",
      "üìã Available functions:\n",
      "   ‚Ä¢ Configuration: Config, ModelConfig, etc.\n",
      "   ‚Ä¢ Models: HuggingFaceRunner, OllamaRunner\n",
      "   ‚Ä¢ Execution: run_red_team_batch, quick_test\n",
      "   ‚Ä¢ Analysis: visualize_results, analyze_top_candidates\n",
      "   ‚Ä¢ Export: export_to_kaggle, create_config_profile\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEPENDENCIES AND IMPORTS (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path (two levels up from testbooks)\n",
    "project_root = os.path.abspath('../..')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import the complete red-teaming framework\n",
    "from redteam import *\n",
    "\n",
    "print(\"üéØ Red Team Framework loaded!\")\n",
    "print(\"üìã Available functions:\")\n",
    "print(\"   ‚Ä¢ Configuration: Config, ModelConfig, etc.\")\n",
    "print(\"   ‚Ä¢ Models: HuggingFaceRunner, OllamaRunner\")\n",
    "print(\"   ‚Ä¢ Execution: run_red_team_batch, quick_test\")\n",
    "print(\"   ‚Ä¢ Analysis: visualize_results, analyze_top_candidates\")\n",
    "print(\"   ‚Ä¢ Export: export_to_kaggle, create_config_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9989d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration created:\n",
      "   Model: openai/gpt-oss-20b\n",
      "   Backend: huggingface\n",
      "   Max tokens: 256\n",
      "   Attempts: 20\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create and customize configuration\n",
    "cfg = Config()\n",
    "\n",
    "# MODEL CONFIGURATION - Update these for your setup\n",
    "cfg.model.model_name = \"openai/gpt-oss-20b\"  # Update with your model path\n",
    "cfg.model.backend = \"huggingface\"  # or \"ollama\"\n",
    "cfg.model.device = \"cuda\"\n",
    "cfg.model.dtype = \"bfloat16\"\n",
    "cfg.model.max_new_tokens = 256  # Start small for testing\n",
    "cfg.model.temperature = 0.2\n",
    "\n",
    "# RUN CONFIGURATION\n",
    "cfg.run.limit_attempts = 20  # Start small for testing\n",
    "cfg.run.out_dir = \"artifacts\"\n",
    "\n",
    "print(f\"‚úÖ Configuration created:\")\n",
    "print(f\"   Model: {cfg.model.model_name}\")\n",
    "print(f\"   Backend: {cfg.model.backend}\")\n",
    "print(f\"   Max tokens: {cfg.model.max_new_tokens}\")\n",
    "print(f\"   Attempts: {cfg.run.limit_attempts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46832480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing red-teaming framework...\n",
      "üìä Framework Status:\n",
      "   ‚ùå runner_initialized: False\n",
      "   ‚ùå seed_families_loaded: False\n",
      "   ‚ùå llm_generator_available: False\n",
      "   ‚úÖ backend: huggingface\n",
      "   ‚úÖ model: openai/gpt-oss-20b\n",
      "üöÄ Initializing Red Team Framework v2.0.0\n",
      "üéØ Model: openai/gpt-oss-20b\n",
      "üîß Backend: huggingface\n",
      "üìù Loaded 27 safety topic families\n",
      "Loading tokenizer: openai/gpt-oss-20b\n",
      "Loading model with dtype: bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c73b78286640eda44941cf91696648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f08629ca8548a0818c8fb25a8ac55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n",
      "üß† LLM variation generator initialized\n",
      "‚úÖ Framework initialization complete!\n",
      "\n",
      "‚úÖ Framework ready!\n",
      "üìù Loaded 27 safety topic families\n",
      "üéØ Ready for red-teaming with huggingface backend\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SYSTEM INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize the framework\n",
    "print(\"üîÑ Initializing red-teaming framework...\")\n",
    "\n",
    "# Check framework status\n",
    "get_framework_status()\n",
    "\n",
    "# Initialize if needed\n",
    "if runner is None:\n",
    "    runner, seed_families = initialize_framework(cfg)\n",
    "\n",
    "print(f\"\\n‚úÖ Framework ready!\")\n",
    "print(f\"üìù Loaded {len(seed_families)} safety topic families\")\n",
    "print(f\"üéØ Ready for red-teaming with {cfg.model.backend} backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1082b16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing HuggingFaceRunner with generate()...\n",
      "Token budget: 8 input + 256 output = 264 total\n",
      "‚úÖ generate() response: <|start|>assistant<|channel|>analysis<|message|>The user says: \"Hello, this is a test prompt.\" They likely want a response. We should respond politely. There's no special instruction. So just greet.<|end|><|start|>assistant<|channel|>final<|message|>Hello! üëã How can I help you today?<|return|>\n",
      "‚úÖ HuggingFaceRunner generate() works!\n",
      "\n",
      "üß™ Testing HuggingFaceRunner with generate_chat()...\n",
      "Token budget: 8 input + 256 output = 264 total\n",
      "‚úÖ generate_chat() response structure: ['prompt_text', 'full_text', 'gen_text', 'input_ids', 'generated_ids', 'model', 'gen_params', 'token_info']\n",
      "\n",
      "üìã Detailed response breakdown:\n",
      "  prompt_text: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-21\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Hello, this is a test prompt.<|end|>\n",
      "  full_text: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-21\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Hello, this is a test prompt.<|end|><|start|>assistant<|channel|>analysis<|message|>The user says: \"Hello, this is a test prompt.\" They likely want a response. We should respond politely. There's no specific instruction. Just greet.<|end|><|start|>assistant<|channel|>final<|message|>Hello! üëã How can I help you today?<|return|>\n",
      "  gen_text: <|start|>assistant<|channel|>analysis<|message|>The user says: \"Hello, this is a test prompt.\" They likely want a response. We should respond politely. There's no specific instruction. Just greet.<|end|><|start|>assistant<|channel|>final<|message|>Hello! üëã How can I help you today?<|return|>\n",
      "  input_ids: [[200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359, 22203, 656, 7788, 17527, 558, 87447, 100594, 25, 220, 1323, 19, 12, 3218, 198, 6576, 3521, 25, 220, 1323, 20, 12, 3062, 12, 2040, 279, 30377, 289, 25, 14093, 279, 2, 13888, 18403, 25, 8450, 11, 49159, 11, 1721, 13, 21030, 2804, 413, 7360, 395, 1753, 3176, 13, 200007, 200006, 1428, 200008, 13225, 11, 495, 382, 261, 1746, 15226, 13, 200007]]\n",
      "  generated_ids: [200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359, 22203, 656, 7788, 17527, 558, 87447, 100594, 25, 220, 1323, 19, 12, 3218, 198, 6576, 3521, 25, 220, 1323, 20, 12, 3062, 12, 2040, 279, 30377, 289, 25, 14093, 279, 2, 13888, 18403, 25, 8450, 11, 49159, 11, 1721, 13, 21030, 2804, 413, 7360, 395, 1753, 3176, 13, 200007, 200006, 1428, 200008, 13225, 11, 495, 382, 261, 1746, 15226, 13, 200007, 200006, 173781, 200005, 35644, 200008, 976, 1825, 5003, 25, 392, 13225, 11, 495, 382, 261, 1746, 15226, 3692, 3164, 6960, 1682, 261, 3322, 13, 1416, 1757, 9570, 167705, 13, 32711, 860, 4857, 21179, 13, 6214, 61400, 13, 200007, 200006, 173781, 200005, 17196, 200008, 13225, 0, 61138, 233, 3253, 665, 357, 1652, 481, 4044, 30, 200002]\n",
      "  model: {'backend': 'huggingface', 'model_name': 'openai/gpt-oss-20b', 'device': 'cuda', 'dtype': 'bfloat16', 'max_new_tokens': 256, 'max_input_tokens': 2048, 'context_window': 4096, 'min_response_tokens': 128, 'temperature': 0.2, 'top_p': 0.9, 'seed': 7, 'use_harmony_chat_template': True, 'add_special_tokens': True, 'dynamic_adjustment': True}\n",
      "  gen_params: {'max_new_tokens': 256, 'configured_max_tokens': 256, 'temperature': 0.2, 'top_p': 0.9, 'seed': 7, 'backend': 'huggingface'}\n",
      "  token_info: {'input_tokens': 8, 'output_tokens': 256, 'response_length_words': 31, 'budget_utilization': 0.064453125, 'dynamic_adjustment': False}\n",
      "‚úÖ HuggingFaceRunner generate_chat() works!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GENERATIVE TEST\n",
    "# =============================================================================\n",
    "# Run a quick test to ensure everything is working\n",
    "\n",
    "# Test with proper message format (FIXED)\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello, this is a test prompt.\"}]\n",
    "\n",
    "print(\"üß™ Testing HuggingFaceRunner with generate()...\")\n",
    "try:\n",
    "    response = runner.generate(test_messages)\n",
    "    print(f\"‚úÖ generate() response: {response}\")\n",
    "    print(\"‚úÖ HuggingFaceRunner generate() works!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå generate() test failed: {type(e).__name__}: {e}\")\n",
    "    print(\"üîß This indicates a configuration or setup issue.\")\n",
    "\n",
    "print(\"\\nüß™ Testing HuggingFaceRunner with generate_chat()...\")\n",
    "try:\n",
    "    response = runner.generate_chat(test_messages)\n",
    "    print(f\"‚úÖ generate_chat() response structure: {list(response.keys())}\")\n",
    "    print(\"\\nüìã Detailed response breakdown:\")\n",
    "    for key, value in response.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"‚úÖ HuggingFaceRunner generate_chat() works!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå generate_chat() test failed: {type(e).__name__}: {e}\")\n",
    "    print(\"üîß This indicates a configuration or setup issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0915257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing HuggingFaceRunner Test Suite...\n",
      "üß™ STARTING COMPREHENSIVE HUGGINGFACE RUNNER TEST SUITE\n",
      "======================================================================\n",
      "üéØ Target Model: openai/gpt-oss-20b\n",
      "üîß Backend: huggingface\n",
      "üíæ Device: cuda\n",
      "üìä Max Tokens: 256\n",
      "======================================================================\n",
      "\n",
      "üîç Test Group 1: Input Validation\n",
      "‚úÖ PASS | 1.1 String input rejection\n",
      "‚úÖ PASS | 1.2 Empty list rejection\n",
      "‚úÖ PASS | 1.3 Invalid message format rejection\n",
      "Token budget: 2 input + 256 output = 258 total\n",
      "‚úÖ PASS | 1.4 Valid message format acceptance\n",
      "\n",
      "üîç Test Group 2: Message Format Handling\n",
      "Token budget: 1 input + 256 output = 257 total\n",
      "‚úÖ PASS | 2.1 Single user message\n",
      "Token budget: 13 input + 256 output = 269 total\n",
      "‚úÖ PASS | 2.2 System + user message\n",
      "Token budget: 11 input + 256 output = 267 total\n",
      "‚úÖ PASS | 2.3 Multi-turn conversation\n",
      "Token budget: 3 input + 256 output = 259 total\n",
      "‚úÖ PASS | 2.4 Message without role field (defaults)\n",
      "\n",
      "üîç Test Group 3: Token Budget Management\n",
      "Token budget: 1 input + 256 output = 257 total\n",
      "‚úÖ PASS | 3.1 Short input processing\n",
      "Token budget: 1401 input + 256 output = 1657 total\n",
      "‚ùå FAIL | 3.2 Long input handling\n",
      "    Details: Error: RuntimeError: Model generation failed: \n",
      "\n",
      "üîç Test Group 4: Generation Parameters\n",
      "Token budget: 6 input + 256 output = 262 total\n",
      "Token budget: 6 input + 256 output = 262 total\n",
      "‚úÖ PASS | 4.1 Temperature parameter handling\n",
      "Token budget: 8 input + 128 output = 136 total\n",
      "‚úÖ PASS | 4.2 Max tokens parameter\n",
      "\n",
      "üîç Test Group 5: Chat Template Handling\n",
      "Token budget: 4 input + 256 output = 260 total\n",
      "Token budget: 4 input + 256 output = 260 total\n",
      "‚úÖ PASS | 5.1 Chat template toggle\n",
      "\n",
      "üîç Test Group 6: Error Handling & Edge Cases\n",
      "Token budget: 22 input + 256 output = 278 total\n",
      "‚úÖ PASS | 6.1 Unicode character handling\n",
      "Token budget: 1 input + 256 output = 257 total\n",
      "‚úÖ PASS | 6.2 Minimal input handling\n",
      "Token budget: 4 input + 256 output = 260 total\n",
      "Token budget: 4 input + 256 output = 260 total\n",
      "Token budget: 4 input + 256 output = 260 total\n",
      "‚úÖ PASS | 6.3 Memory usage stability\n",
      "\n",
      "üîç Test Group 7: Performance Benchmarks\n",
      "Token budget: 7 input + 256 output = 263 total\n",
      "‚úÖ PASS | 7.1 Generation speed\n",
      "\n",
      "üèÅ TEST SUITE COMPLETE\n",
      "======================================================================\n",
      "‚è±Ô∏è  Total runtime: 72.80 seconds\n",
      "üìä Tests run: 17\n",
      "‚úÖ Passed: 16\n",
      "‚ùå Failed: 1\n",
      "‚ö†Ô∏è  1 test(s) failed. See details above.\n",
      "üîß Failed tests: 3.2 Long input handling\n",
      "üìà Success rate: 94.1%\n",
      "\n",
      "üèÜ EXCELLENT: HuggingFaceRunner passes professional testing standards!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE HUGGINGFACE RUNNER TEST SUITE\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class HuggingFaceRunnerTester:\n",
    "    \"\"\"Professional test suite for HuggingFaceRunner with comprehensive coverage\"\"\"\n",
    "    \n",
    "    def __init__(self, runner, cfg):\n",
    "        self.runner = runner\n",
    "        self.cfg = cfg\n",
    "        self.test_results = []\n",
    "        self.failed_tests = []\n",
    "        \n",
    "    def log_test(self, test_name: str, passed: bool, details: str = \"\"):\n",
    "        \"\"\"Log test results\"\"\"\n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        self.test_results.append({\n",
    "            'test': test_name,\n",
    "            'passed': passed,\n",
    "            'details': details\n",
    "        })\n",
    "        print(f\"{status} | {test_name}\")\n",
    "        if details and not passed:\n",
    "            print(f\"    Details: {details}\")\n",
    "        if not passed:\n",
    "            self.failed_tests.append(test_name)\n",
    "\n",
    "    def test_input_validation(self) -> bool:\n",
    "        \"\"\"Test 1: Input validation and error handling\"\"\"\n",
    "        print(\"\\nüîç Test Group 1: Input Validation\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 1.1: String input (should fail)\n",
    "        try:\n",
    "            self.runner.generate(\"This is a string, not a list\")\n",
    "            self.log_test(\"1.1 String input rejection\", False, \"Should have raised TypeError\")\n",
    "            all_passed = False\n",
    "        except TypeError as e:\n",
    "            if \"Expected List[Dict[str, str]]\" in str(e):\n",
    "                self.log_test(\"1.1 String input rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.1 String input rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.1 String input rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.2: Empty list (should fail)\n",
    "        try:\n",
    "            self.runner.generate([])\n",
    "            self.log_test(\"1.2 Empty list rejection\", False, \"Should have raised ValueError\")\n",
    "            all_passed = False\n",
    "        except ValueError as e:\n",
    "            if \"cannot be empty\" in str(e):\n",
    "                self.log_test(\"1.2 Empty list rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.2 Empty list rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.2 Empty list rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.3: Invalid message format (should fail)\n",
    "        try:\n",
    "            self.runner.generate([{\"invalid\": \"format\"}])\n",
    "            self.log_test(\"1.3 Invalid message format rejection\", False, \"Should have raised ValueError\")\n",
    "            all_passed = False\n",
    "        except ValueError as e:\n",
    "            if \"missing required 'content' field\" in str(e):\n",
    "                self.log_test(\"1.3 Invalid message format rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.3 Invalid message format rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.3 Invalid message format rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.4: Valid message format (should pass)\n",
    "        try:\n",
    "            valid_messages = [{\"role\": \"user\", \"content\": \"Test message\"}]\n",
    "            result = self.runner.generate(valid_messages)\n",
    "            if isinstance(result, str) and len(result) > 0:\n",
    "                self.log_test(\"1.4 Valid message format acceptance\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.4 Valid message format acceptance\", False, f\"Invalid response type or empty: {type(result)}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.4 Valid message format acceptance\", False, f\"Unexpected error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_message_formats(self) -> bool:\n",
    "        \"\"\"Test 2: Different message format scenarios\"\"\"\n",
    "        print(\"\\nüîç Test Group 2: Message Format Handling\")\n",
    "        all_passed = True\n",
    "        \n",
    "        test_cases = [\n",
    "            {\n",
    "                \"name\": \"2.1 Single user message\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.2 System + user message\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.3 Multi-turn conversation\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"Tell me about AI\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"AI is artificial intelligence\"},\n",
    "                    {\"role\": \"user\", \"content\": \"Tell me more\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.4 Message without role field (defaults)\",\n",
    "                \"messages\": [{\"content\": \"Test without role\"}]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for case in test_cases:\n",
    "            try:\n",
    "                result = self.runner.generate(case[\"messages\"])\n",
    "                if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                    self.log_test(case[\"name\"], True)\n",
    "                else:\n",
    "                    self.log_test(case[\"name\"], False, f\"Empty or invalid response: '{result}'\")\n",
    "                    all_passed = False\n",
    "            except Exception as e:\n",
    "                self.log_test(case[\"name\"], False, f\"Error: {type(e).__name__}: {e}\")\n",
    "                all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_token_budget_handling(self) -> bool:\n",
    "        \"\"\"Test 3: Token budget and context management\"\"\"\n",
    "        print(\"\\nüîç Test Group 3: Token Budget Management\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 3.1: Short input (should work normally)\n",
    "        try:\n",
    "            short_msg = [{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "            result = self.runner.generate_chat(short_msg)\n",
    "            \n",
    "            # Validate response structure\n",
    "            required_fields = [\"gen_text\", \"token_info\", \"gen_params\"]\n",
    "            missing_fields = [f for f in required_fields if f not in result]\n",
    "            \n",
    "            if not missing_fields:\n",
    "                self.log_test(\"3.1 Short input processing\", True)\n",
    "            else:\n",
    "                self.log_test(\"3.1 Short input processing\", False, f\"Missing fields: {missing_fields}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"3.1 Short input processing\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 3.2: Very long input (should handle gracefully)\n",
    "        try:\n",
    "            long_content = \"This is a very long message. \" * 200  # ~1200 words\n",
    "            long_msg = [{\"role\": \"user\", \"content\": long_content}]\n",
    "            result = self.runner.generate_chat(long_msg)\n",
    "            \n",
    "            # Should still work but may have adjusted token budget\n",
    "            if \"gen_text\" in result and \"token_info\" in result:\n",
    "                token_info = result[\"token_info\"]\n",
    "                if \"dynamic_adjustment\" in token_info:\n",
    "                    self.log_test(\"3.2 Long input handling\", True, \n",
    "                                f\"Dynamic adjustment: {token_info.get('dynamic_adjustment', 'N/A')}\")\n",
    "                else:\n",
    "                    self.log_test(\"3.2 Long input handling\", True)\n",
    "            else:\n",
    "                self.log_test(\"3.2 Long input handling\", False, \"Missing required response fields\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"3.2 Long input handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_generation_parameters(self) -> bool:\n",
    "        \"\"\"Test 4: Generation parameter handling\"\"\"\n",
    "        print(\"\\nüîç Test Group 4: Generation Parameters\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 4.1: Temperature effects\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Generate a creative story opening.\"}]\n",
    "            \n",
    "            # Store original temperature\n",
    "            original_temp = self.cfg.model.temperature\n",
    "            \n",
    "            # Test with low temperature\n",
    "            self.cfg.model.temperature = 0.1\n",
    "            result_low = self.runner.generate(msg)\n",
    "            \n",
    "            # Test with high temperature  \n",
    "            self.cfg.model.temperature = 0.9\n",
    "            result_high = self.runner.generate(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.temperature = original_temp\n",
    "            \n",
    "            # Both should succeed\n",
    "            if isinstance(result_low, str) and isinstance(result_high, str):\n",
    "                self.log_test(\"4.1 Temperature parameter handling\", True,\n",
    "                            f\"Low temp: {len(result_low)} chars, High temp: {len(result_high)} chars\")\n",
    "            else:\n",
    "                self.log_test(\"4.1 Temperature parameter handling\", False, \"Non-string results\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Restore original temperature on error\n",
    "            self.cfg.model.temperature = original_temp\n",
    "            self.log_test(\"4.1 Temperature parameter handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 4.2: Max tokens parameter\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Write a detailed explanation of machine learning.\"}]\n",
    "            \n",
    "            original_max_tokens = self.cfg.model.max_new_tokens\n",
    "            \n",
    "            # Test with very small token limit\n",
    "            self.cfg.model.max_new_tokens = 10\n",
    "            result = self.runner.generate_chat(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.max_new_tokens = original_max_tokens\n",
    "            \n",
    "            if \"gen_text\" in result and len(result[\"gen_text\"].strip()) > 0:\n",
    "                self.log_test(\"4.2 Max tokens parameter\", True, \n",
    "                            f\"Generated {len(result['gen_text'].split())} words with limit 10 tokens\")\n",
    "            else:\n",
    "                self.log_test(\"4.2 Max tokens parameter\", False, \"Empty or invalid result\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.cfg.model.max_new_tokens = original_max_tokens\n",
    "            self.log_test(\"4.2 Max tokens parameter\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_chat_template_handling(self) -> bool:\n",
    "        \"\"\"Test 5: Chat template functionality\"\"\"\n",
    "        print(\"\\nüîç Test Group 5: Chat Template Handling\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 5.1: Harmony chat template enabled\n",
    "        try:\n",
    "            original_harmony = self.cfg.model.use_harmony_chat_template\n",
    "            \n",
    "            self.cfg.model.use_harmony_chat_template = True\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Test with harmony template\"}]\n",
    "            result_harmony = self.runner.generate_chat(msg)\n",
    "            \n",
    "            self.cfg.model.use_harmony_chat_template = False\n",
    "            result_no_harmony = self.runner.generate_chat(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.use_harmony_chat_template = original_harmony\n",
    "            \n",
    "            # Both should work\n",
    "            if (\"gen_text\" in result_harmony and \"gen_text\" in result_no_harmony and\n",
    "                len(result_harmony[\"gen_text\"].strip()) > 0 and len(result_no_harmony[\"gen_text\"].strip()) > 0):\n",
    "                self.log_test(\"5.1 Chat template toggle\", True,\n",
    "                            f\"Harmony: {len(result_harmony['gen_text'])} chars, No harmony: {len(result_no_harmony['gen_text'])} chars\")\n",
    "            else:\n",
    "                self.log_test(\"5.1 Chat template toggle\", False, \"Invalid results from template comparison\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.cfg.model.use_harmony_chat_template = original_harmony\n",
    "            self.log_test(\"5.1 Chat template toggle\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_error_handling_robustness(self) -> bool:\n",
    "        \"\"\"Test 6: Error handling and edge cases\"\"\"\n",
    "        print(\"\\nüîç Test Group 6: Error Handling & Edge Cases\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 6.1: Unicode and special characters\n",
    "        try:\n",
    "            unicode_msg = [{\"role\": \"user\", \"content\": \"Hello! üëã Can you handle √©mojis and sp√´ci√°l ch√¢ract√´rs? ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\"}]\n",
    "            result = self.runner.generate(unicode_msg)\n",
    "            if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                self.log_test(\"6.1 Unicode character handling\", True)\n",
    "            else:\n",
    "                self.log_test(\"6.1 Unicode character handling\", False, \"Invalid unicode response\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.1 Unicode character handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 6.2: Very short content\n",
    "        try:\n",
    "            short_msg = [{\"role\": \"user\", \"content\": \"?\"}]\n",
    "            result = self.runner.generate(short_msg)\n",
    "            if isinstance(result, str):\n",
    "                self.log_test(\"6.2 Minimal input handling\", True, f\"Response: '{result[:50]}...'\")\n",
    "            else:\n",
    "                self.log_test(\"6.2 Minimal input handling\", False, \"Invalid response type\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.2 Minimal input handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 6.3: Memory cleanup (basic check)\n",
    "        try:\n",
    "            initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            \n",
    "            # Generate several responses\n",
    "            for i in range(3):\n",
    "                msg = [{\"role\": \"user\", \"content\": f\"Memory test {i}\"}]\n",
    "                _ = self.runner.generate(msg)\n",
    "            \n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            final_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            memory_diff = final_memory - initial_memory\n",
    "            \n",
    "            # Memory should not grow excessively\n",
    "            self.log_test(\"6.3 Memory usage stability\", True, \n",
    "                         f\"Memory change: {memory_diff:,} bytes ({memory_diff/1024/1024:.2f} MB)\")\n",
    "                         \n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.3 Memory usage stability\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_performance_benchmarks(self) -> bool:\n",
    "        \"\"\"Test 7: Basic performance benchmarks\"\"\"\n",
    "        print(\"\\nüîç Test Group 7: Performance Benchmarks\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 7.1: Generation speed\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Write a short paragraph about technology.\"}]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = self.runner.generate(msg)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            generation_time = end_time - start_time\n",
    "            tokens_generated = len(result.split()) if isinstance(result, str) else 0\n",
    "            tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0\n",
    "            \n",
    "            if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                self.log_test(\"7.1 Generation speed\", True, \n",
    "                             f\"Time: {generation_time:.2f}s, Tokens: {tokens_generated}, Speed: {tokens_per_second:.1f} tok/s\")\n",
    "            else:\n",
    "                self.log_test(\"7.1 Generation speed\", False, \"Invalid generation result\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_test(\"7.1 Generation speed\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def run_all_tests(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run the complete test suite\"\"\"\n",
    "        print(\"üß™ STARTING COMPREHENSIVE HUGGINGFACE RUNNER TEST SUITE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üéØ Target Model: {self.cfg.model.model_name}\")\n",
    "        print(f\"üîß Backend: {self.cfg.model.backend}\")\n",
    "        print(f\"üíæ Device: {self.cfg.model.device}\")\n",
    "        print(f\"üìä Max Tokens: {self.cfg.model.max_new_tokens}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run all test groups\n",
    "        test_groups = [\n",
    "            (\"Input Validation\", self.test_input_validation),\n",
    "            (\"Message Formats\", self.test_message_formats),\n",
    "            (\"Token Budget\", self.test_token_budget_handling),\n",
    "            (\"Generation Parameters\", self.test_generation_parameters),\n",
    "            (\"Chat Templates\", self.test_chat_template_handling),\n",
    "            (\"Error Handling\", self.test_error_handling_robustness),\n",
    "            (\"Performance\", self.test_performance_benchmarks)\n",
    "        ]\n",
    "        \n",
    "        group_results = {}\n",
    "        for group_name, test_func in test_groups:\n",
    "            try:\n",
    "                group_results[group_name] = test_func()\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå CRITICAL ERROR in {group_name}: {type(e).__name__}: {e}\")\n",
    "                group_results[group_name] = False\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Generate summary\n",
    "        print(f\"\\nüèÅ TEST SUITE COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"‚è±Ô∏è  Total runtime: {total_time:.2f} seconds\")\n",
    "        print(f\"üìä Tests run: {len(self.test_results)}\")\n",
    "        \n",
    "        passed_count = sum(1 for result in self.test_results if result['passed'])\n",
    "        failed_count = len(self.test_results) - passed_count\n",
    "        \n",
    "        print(f\"‚úÖ Passed: {passed_count}\")\n",
    "        print(f\"‚ùå Failed: {failed_count}\")\n",
    "        \n",
    "        if failed_count == 0:\n",
    "            print(\"üéâ ALL TESTS PASSED - HuggingFaceRunner is working correctly!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {failed_count} test(s) failed. See details above.\")\n",
    "            print(f\"üîß Failed tests: {', '.join(self.failed_tests)}\")\n",
    "        \n",
    "        success_rate = (passed_count / len(self.test_results)) * 100 if self.test_results else 0\n",
    "        print(f\"üìà Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"total_tests\": len(self.test_results),\n",
    "            \"passed\": passed_count,\n",
    "            \"failed\": failed_count,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"runtime\": total_time,\n",
    "            \"failed_tests\": self.failed_tests,\n",
    "            \"group_results\": group_results,\n",
    "            \"detailed_results\": self.test_results\n",
    "        }\n",
    "\n",
    "# Execute the comprehensive test suite\n",
    "print(\"üöÄ Initializing HuggingFaceRunner Test Suite...\")\n",
    "\n",
    "if 'runner' in globals() and runner is not None:\n",
    "    tester = HuggingFaceRunnerTester(runner, cfg)\n",
    "    test_summary = tester.run_all_tests()\n",
    "    \n",
    "    # Store test results for future reference\n",
    "    globals()['hf_runner_test_results'] = test_summary\n",
    "    \n",
    "    if test_summary['success_rate'] >= 90:\n",
    "        print(\"\\nüèÜ EXCELLENT: HuggingFaceRunner passes professional testing standards!\")\n",
    "    elif test_summary['success_rate'] >= 75:\n",
    "        print(\"\\nüëç GOOD: HuggingFaceRunner is functional with minor issues.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  ATTENTION NEEDED: Multiple test failures require investigation.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Runner not initialized. Please run the framework initialization cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red_team_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
