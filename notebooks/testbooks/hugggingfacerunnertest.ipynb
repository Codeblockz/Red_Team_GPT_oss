{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2ftgmjs11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MEMORY OPTIMIZATION AND TROUBLESHOOTING\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory\n",
    "print(\"üßπ Clearing GPU memory...\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check current memory usage\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    free = total_memory - allocated\n",
    "    \n",
    "    print(f\"üìä GPU Memory Status:\")\n",
    "    print(f\"   Total: {total_memory:.2f} GB\")\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB\") \n",
    "    print(f\"   Free: {free:.2f} GB\")\n",
    "    \n",
    "    if allocated > 25:  # More than 25GB in use\n",
    "        print(\"‚ö†Ô∏è  High memory usage detected!\")\n",
    "        print(\"üîß Suggestions:\")\n",
    "        print(\"   1. Restart kernel to clear all memory\")\n",
    "        print(\"   2. Use CPU offloading\")\n",
    "        print(\"   3. Use 8-bit or 4-bit quantization\")\n",
    "        print(\"   4. Use smaller model variants\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353cee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"openai/gpt-oss-20b\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# prompt_low = \"Question: What is 25 * 32?\\nAnswer:\"\n",
    "# prompt_medium = \"Question: What is 25 * 32?\\nLet's think step by step.\\nAnswer:\"\n",
    "# prompt_high = \"\"\"{\"id\":\"abc123\",\"conversations\":[\n",
    "# {\"from\":\"user\",\"value\":\"What is 25 * 32?\"},\n",
    "# {\"from\":\"assistant\",\"value\":\"Let's think carefully.\",\"scratchpad\":\"...\"}\n",
    "# ]}\"\"\"\n",
    "\n",
    "# inputs = tokenizer(prompt_medium, return_tensors=\"pt\").to(\"cuda\")\n",
    "# outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEPENDENCIES AND IMPORTS (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path (two levels up from testbooks)\n",
    "project_root = os.path.abspath('../..')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import the complete red-teaming framework\n",
    "from redteam import *\n",
    "\n",
    "print(\"üéØ Red Team Framework loaded!\")\n",
    "print(\"üìã Available functions:\")\n",
    "print(\"   ‚Ä¢ Configuration: Config, ModelConfig, etc.\")\n",
    "print(\"   ‚Ä¢ Models: HuggingFaceRunner, OllamaRunner\")\n",
    "print(\"   ‚Ä¢ Execution: run_red_team_batch, quick_test\")\n",
    "print(\"   ‚Ä¢ Analysis: visualize_results, analyze_top_candidates\")\n",
    "print(\"   ‚Ä¢ Export: export_to_kaggle, create_config_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9989d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create and customize configuration\n",
    "cfg = Config()\n",
    "\n",
    "# MODEL CONFIGURATION - Update these for your setup\n",
    "cfg.model.model_name = \"openai/gpt-oss-20b\"  # Update with your model path\n",
    "cfg.model.backend = \"huggingface\"  # or \"ollama\"\n",
    "cfg.model.device = \"cuda\"\n",
    "cfg.model.dtype = \"bfloat16\"\n",
    "cfg.model.max_new_tokens = 256  # Start small for testing\n",
    "cfg.model.temperature = 0.2\n",
    "\n",
    "# RUN CONFIGURATION\n",
    "cfg.run.limit_attempts = 20  # Start small for testing\n",
    "cfg.run.out_dir = \"artifacts\"\n",
    "\n",
    "print(f\"‚úÖ Configuration created:\")\n",
    "print(f\"   Model: {cfg.model.model_name}\")\n",
    "print(f\"   Backend: {cfg.model.backend}\")\n",
    "print(f\"   Max tokens: {cfg.model.max_new_tokens}\")\n",
    "print(f\"   Attempts: {cfg.run.limit_attempts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46832480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SYSTEM INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize the framework\n",
    "print(\"üîÑ Initializing red-teaming framework...\")\n",
    "\n",
    "# Check framework status\n",
    "get_framework_status()\n",
    "\n",
    "# Initialize if needed\n",
    "if runner is None:\n",
    "    runner, seed_families = initialize_framework(cfg)\n",
    "\n",
    "print(f\"\\n‚úÖ Framework ready!\")\n",
    "print(f\"üìù Loaded {len(seed_families)} safety topic families\")\n",
    "print(f\"üéØ Ready for red-teaming with {cfg.model.backend} backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATIVE TEST\n",
    "# =============================================================================\n",
    "# Run a quick test to ensure everything is working\n",
    "\n",
    "# Test with proper message format (FIXED)\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello, this is a test prompt.\"}]\n",
    "\n",
    "print(\"üß™ Testing HuggingFaceRunner with generate()...\")\n",
    "try:\n",
    "    response = runner.generate(test_messages)\n",
    "    print(f\"‚úÖ generate() response: {response}\")\n",
    "    print(\"‚úÖ HuggingFaceRunner generate() works!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå generate() test failed: {type(e).__name__}: {e}\")\n",
    "    print(\"üîß This indicates a configuration or setup issue.\")\n",
    "\n",
    "print(\"\\nüß™ Testing HuggingFaceRunner with generate_chat()...\")\n",
    "try:\n",
    "    response = runner.generate_chat(test_messages)\n",
    "    print(f\"‚úÖ generate_chat() response structure: {list(response.keys())}\")\n",
    "    print(\"\\nüìã Detailed response breakdown:\")\n",
    "    for key, value in response.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"‚úÖ HuggingFaceRunner generate_chat() works!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå generate_chat() test failed: {type(e).__name__}: {e}\")\n",
    "    print(\"üîß This indicates a configuration or setup issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d70d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_content = \"This is a very long message. \" * 200  # ~1200 words\n",
    "test_messages= [{\"role\": \"user\", \"content\": long_content}]\n",
    "\n",
    "print(\"\\nüß™ Testing long msg HuggingFaceRunner with generate_chat()...\")\n",
    "try:\n",
    "    response = runner.generate_chat(test_messages)\n",
    "    print(f\"‚úÖ generate_chat() response structure: {list(response.keys())}\")\n",
    "    print(\"\\nüìã Detailed response breakdown:\")\n",
    "    for key, value in response.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"‚úÖ HuggingFaceRunner generate_chat() works!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå generate_chat() test failed: {type(e).__name__}: {e}\")\n",
    "    print(\"üîß This indicates a configuration or setup issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617333ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model response:\", response['gen_text'].split(\"<|channel|>final<|message|>\")[-1].strip())\n",
    "print(\"Full Generated Text:\", response['gen_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0915257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test-Driven Development for Phase 3: Multi-Turn Conversation Framework\n",
    "\n",
    "This file tests each component as we build it to ensure quality before notebook integration.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class HuggingFaceRunnerTester:\n",
    "    \"\"\"Professional test suite for HuggingFaceRunner with comprehensive coverage\"\"\"\n",
    "    \n",
    "    def __init__(self, runner, cfg):\n",
    "        self.runner = runner\n",
    "        self.cfg = cfg\n",
    "        self.test_results = []\n",
    "        self.failed_tests = []\n",
    "        \n",
    "    def log_test(self, test_name: str, passed: bool, details: str = \"\"):\n",
    "        \"\"\"Log test results\"\"\"\n",
    "        status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "        self.test_results.append({\n",
    "            'test': test_name,\n",
    "            'passed': passed,\n",
    "            'details': details\n",
    "        })\n",
    "        print(f\"{status} | {test_name}\")\n",
    "        if details and not passed:\n",
    "            print(f\"    Details: {details}\")\n",
    "        if not passed:\n",
    "            self.failed_tests.append(test_name)\n",
    "\n",
    "    def test_input_validation(self) -> bool:\n",
    "        \"\"\"Test 1: Input validation and error handling\"\"\"\n",
    "        print(\"\\nüîç Test Group 1: Input Validation\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 1.1: String input (should fail)\n",
    "        try:\n",
    "            self.runner.generate(\"This is a string, not a list\")\n",
    "            self.log_test(\"1.1 String input rejection\", False, \"Should have raised TypeError\")\n",
    "            all_passed = False\n",
    "        except TypeError as e:\n",
    "            if \"Expected List[Dict[str, str]]\" in str(e):\n",
    "                self.log_test(\"1.1 String input rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.1 String input rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.1 String input rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.2: Empty list (should fail)\n",
    "        try:\n",
    "            self.runner.generate([])\n",
    "            self.log_test(\"1.2 Empty list rejection\", False, \"Should have raised ValueError\")\n",
    "            all_passed = False\n",
    "        except ValueError as e:\n",
    "            if \"cannot be empty\" in str(e):\n",
    "                self.log_test(\"1.2 Empty list rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.2 Empty list rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.2 Empty list rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.3: Invalid message format (should fail)\n",
    "        try:\n",
    "            self.runner.generate([{\"invalid\": \"format\"}])\n",
    "            self.log_test(\"1.3 Invalid message format rejection\", False, \"Should have raised ValueError\")\n",
    "            all_passed = False\n",
    "        except ValueError as e:\n",
    "            if \"missing required 'content' field\" in str(e):\n",
    "                self.log_test(\"1.3 Invalid message format rejection\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.3 Invalid message format rejection\", False, f\"Wrong error message: {e}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.3 Invalid message format rejection\", False, f\"Wrong exception type: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 1.4: Valid message format (should pass)\n",
    "        try:\n",
    "            valid_messages = [{\"role\": \"user\", \"content\": \"Test message\"}]\n",
    "            result = self.runner.generate(valid_messages)\n",
    "            if isinstance(result, str) and len(result) > 0:\n",
    "                self.log_test(\"1.4 Valid message format acceptance\", True)\n",
    "            else:\n",
    "                self.log_test(\"1.4 Valid message format acceptance\", False, f\"Invalid response type or empty: {type(result)}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"1.4 Valid message format acceptance\", False, f\"Unexpected error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_message_formats(self) -> bool:\n",
    "        \"\"\"Test 2: Different message format scenarios\"\"\"\n",
    "        print(\"\\nüîç Test Group 2: Message Format Handling\")\n",
    "        all_passed = True\n",
    "        \n",
    "        test_cases = [\n",
    "            {\n",
    "                \"name\": \"2.1 Single user message\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.2 System + user message\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.3 Multi-turn conversation\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"Tell me about AI\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"AI is artificial intelligence\"},\n",
    "                    {\"role\": \"user\", \"content\": \"Tell me more\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"2.4 Message without role field (defaults)\",\n",
    "                \"messages\": [{\"content\": \"Test without role\"}]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for case in test_cases:\n",
    "            try:\n",
    "                result = self.runner.generate(case[\"messages\"])\n",
    "                if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                    self.log_test(case[\"name\"], True)\n",
    "                else:\n",
    "                    self.log_test(case[\"name\"], False, f\"Empty or invalid response: '{result}'\")\n",
    "                    all_passed = False\n",
    "            except Exception as e:\n",
    "                self.log_test(case[\"name\"], False, f\"Error: {type(e).__name__}: {e}\")\n",
    "                all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_token_budget_handling(self) -> bool:\n",
    "        \"\"\"Test 3: Token budget and context management\"\"\"\n",
    "        print(\"\\nüîç Test Group 3: Token Budget Management\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 3.1: Short input (should work normally)\n",
    "        try:\n",
    "            short_msg = [{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "            result = self.runner.generate_chat(short_msg)\n",
    "            \n",
    "            # Validate response structure\n",
    "            required_fields = [\"gen_text\", \"token_info\", \"gen_params\"]\n",
    "            missing_fields = [f for f in required_fields if f not in result]\n",
    "            \n",
    "            if not missing_fields:\n",
    "                self.log_test(\"3.1 Short input processing\", True)\n",
    "            else:\n",
    "                self.log_test(\"3.1 Short input processing\", False, f\"Missing fields: {missing_fields}\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"3.1 Short input processing\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 3.2: Very long input (should handle gracefully)\n",
    "        try:\n",
    "            long_content = \"This is a very long message. \" * 200  # ~1200 words\n",
    "            long_msg = [{\"role\": \"user\", \"content\": long_content}]\n",
    "            result = self.runner.generate_chat(long_msg)\n",
    "            \n",
    "            # Should still work but may have adjusted token budget\n",
    "            if \"gen_text\" in result and \"token_info\" in result:\n",
    "                token_info = result[\"token_info\"]\n",
    "                if \"dynamic_adjustment\" in token_info:\n",
    "                    self.log_test(\"3.2 Long input handling\", True, \n",
    "                                f\"Dynamic adjustment: {token_info.get('dynamic_adjustment', 'N/A')}\")\n",
    "                else:\n",
    "                    self.log_test(\"3.2 Long input handling\", True)\n",
    "            else:\n",
    "                self.log_test(\"3.2 Long input handling\", False, \"Missing required response fields\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"3.2 Long input handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_generation_parameters(self) -> bool:\n",
    "        \"\"\"Test 4: Generation parameter handling\"\"\"\n",
    "        print(\"\\nüîç Test Group 4: Generation Parameters\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 4.1: Temperature effects\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Generate a creative story opening.\"}]\n",
    "            \n",
    "            # Store original temperature\n",
    "            original_temp = self.cfg.model.temperature\n",
    "            \n",
    "            # Test with low temperature\n",
    "            self.cfg.model.temperature = 0.1\n",
    "            result_low = self.runner.generate(msg)\n",
    "            \n",
    "            # Test with high temperature  \n",
    "            self.cfg.model.temperature = 0.9\n",
    "            result_high = self.runner.generate(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.temperature = original_temp\n",
    "            \n",
    "            # Both should succeed\n",
    "            if isinstance(result_low, str) and isinstance(result_high, str):\n",
    "                self.log_test(\"4.1 Temperature parameter handling\", True,\n",
    "                            f\"Low temp: {len(result_low)} chars, High temp: {len(result_high)} chars\")\n",
    "            else:\n",
    "                self.log_test(\"4.1 Temperature parameter handling\", False, \"Non-string results\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Restore original temperature on error\n",
    "            self.cfg.model.temperature = original_temp\n",
    "            self.log_test(\"4.1 Temperature parameter handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 4.2: Max tokens parameter\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Write a detailed explanation of machine learning.\"}]\n",
    "            \n",
    "            original_max_tokens = self.cfg.model.max_new_tokens\n",
    "            \n",
    "            # Test with very small token limit\n",
    "            self.cfg.model.max_new_tokens = 10\n",
    "            result = self.runner.generate_chat(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.max_new_tokens = original_max_tokens\n",
    "            \n",
    "            if \"gen_text\" in result and len(result[\"gen_text\"].strip()) > 0:\n",
    "                self.log_test(\"4.2 Max tokens parameter\", True, \n",
    "                            f\"Generated {len(result['gen_text'].split())} words with limit 10 tokens\")\n",
    "            else:\n",
    "                self.log_test(\"4.2 Max tokens parameter\", False, \"Empty or invalid result\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.cfg.model.max_new_tokens = original_max_tokens\n",
    "            self.log_test(\"4.2 Max tokens parameter\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_chat_template_handling(self) -> bool:\n",
    "        \"\"\"Test 5: Chat template functionality\"\"\"\n",
    "        print(\"\\nüîç Test Group 5: Chat Template Handling\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 5.1: Harmony chat template enabled\n",
    "        try:\n",
    "            original_harmony = self.cfg.model.use_harmony_chat_template\n",
    "            \n",
    "            self.cfg.model.use_harmony_chat_template = True\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Test with harmony template\"}]\n",
    "            result_harmony = self.runner.generate_chat(msg)\n",
    "            \n",
    "            self.cfg.model.use_harmony_chat_template = False\n",
    "            result_no_harmony = self.runner.generate_chat(msg)\n",
    "            \n",
    "            # Restore original\n",
    "            self.cfg.model.use_harmony_chat_template = original_harmony\n",
    "            \n",
    "            # Both should work\n",
    "            if (\"gen_text\" in result_harmony and \"gen_text\" in result_no_harmony and\n",
    "                len(result_harmony[\"gen_text\"].strip()) > 0 and len(result_no_harmony[\"gen_text\"].strip()) > 0):\n",
    "                self.log_test(\"5.1 Chat template toggle\", True,\n",
    "                            f\"Harmony: {len(result_harmony['gen_text'])} chars, No harmony: {len(result_no_harmony['gen_text'])} chars\")\n",
    "            else:\n",
    "                self.log_test(\"5.1 Chat template toggle\", False, \"Invalid results from template comparison\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.cfg.model.use_harmony_chat_template = original_harmony\n",
    "            self.log_test(\"5.1 Chat template toggle\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_error_handling_robustness(self) -> bool:\n",
    "        \"\"\"Test 6: Error handling and edge cases\"\"\"\n",
    "        print(\"\\nüîç Test Group 6: Error Handling & Edge Cases\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 6.1: Unicode and special characters\n",
    "        try:\n",
    "            unicode_msg = [{\"role\": \"user\", \"content\": \"Hello! üëã Can you handle √©mojis and sp√´ci√°l ch√¢ract√´rs? ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\"}]\n",
    "            result = self.runner.generate(unicode_msg)\n",
    "            if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                self.log_test(\"6.1 Unicode character handling\", True)\n",
    "            else:\n",
    "                self.log_test(\"6.1 Unicode character handling\", False, \"Invalid unicode response\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.1 Unicode character handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 6.2: Very short content\n",
    "        try:\n",
    "            short_msg = [{\"role\": \"user\", \"content\": \"?\"}]\n",
    "            result = self.runner.generate(short_msg)\n",
    "            if isinstance(result, str):\n",
    "                self.log_test(\"6.2 Minimal input handling\", True, f\"Response: '{result[:50]}...'\")\n",
    "            else:\n",
    "                self.log_test(\"6.2 Minimal input handling\", False, \"Invalid response type\")\n",
    "                all_passed = False\n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.2 Minimal input handling\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 6.3: Memory cleanup (basic check)\n",
    "        try:\n",
    "            initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            \n",
    "            # Generate several responses\n",
    "            for i in range(3):\n",
    "                msg = [{\"role\": \"user\", \"content\": f\"Memory test {i}\"}]\n",
    "                _ = self.runner.generate(msg)\n",
    "            \n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            final_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "            memory_diff = final_memory - initial_memory\n",
    "            \n",
    "            # Memory should not grow excessively\n",
    "            self.log_test(\"6.3 Memory usage stability\", True, \n",
    "                         f\"Memory change: {memory_diff:,} bytes ({memory_diff/1024/1024:.2f} MB)\")\n",
    "                         \n",
    "        except Exception as e:\n",
    "            self.log_test(\"6.3 Memory usage stability\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_performance_benchmarks(self) -> bool:\n",
    "        \"\"\"Test 7: Basic performance benchmarks\"\"\"\n",
    "        print(\"\\nüîç Test Group 7: Performance Benchmarks\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Test 7.1: Generation speed\n",
    "        try:\n",
    "            msg = [{\"role\": \"user\", \"content\": \"Write a short paragraph about technology.\"}]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = self.runner.generate(msg)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            generation_time = end_time - start_time\n",
    "            tokens_generated = len(result.split()) if isinstance(result, str) else 0\n",
    "            tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0\n",
    "            \n",
    "            if isinstance(result, str) and len(result.strip()) > 0:\n",
    "                self.log_test(\"7.1 Generation speed\", True, \n",
    "                             f\"Time: {generation_time:.2f}s, Tokens: {tokens_generated}, Speed: {tokens_per_second:.1f} tok/s\")\n",
    "            else:\n",
    "                self.log_test(\"7.1 Generation speed\", False, \"Invalid generation result\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_test(\"7.1 Generation speed\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def test_multi_turn_conversations(self) -> bool:\n",
    "        \"\"\"Test 8: Multi-turn conversation functionality\"\"\"\n",
    "        print(\"\\nüîç Test Group 8: Multi-turn Conversations\")\n",
    "        all_passed = True\n",
    "        \n",
    "        # Import ConversationManager for testing\n",
    "        try:\n",
    "            from redteam.test_conversation_framework import ConversationManager\n",
    "        except ImportError:\n",
    "            # Create minimal mock if import fails\n",
    "            class ConversationManager:\n",
    "                def __init__(self, max_turns=3):\n",
    "                    self.max_turns = max_turns\n",
    "                    self.conversations = {}\n",
    "                \n",
    "                def create_conversation(self, seed_prompt):\n",
    "                    import uuid\n",
    "                    conv_id = f\"conv_{uuid.uuid4().hex[:8]}\"\n",
    "                    self.conversations[conv_id] = {\n",
    "                        'id': conv_id, 'turns': [], 'seed': seed_prompt\n",
    "                    }\n",
    "                    return conv_id\n",
    "                \n",
    "                def add_turn(self, conv_id, prompt, response):\n",
    "                    if conv_id in self.conversations:\n",
    "                        self.conversations[conv_id]['turns'].append({\n",
    "                            'prompt': prompt, 'response': response\n",
    "                        })\n",
    "        \n",
    "        # Test 8.1: Basic 2-turn conversation flow\n",
    "        try:\n",
    "            conversation_manager = ConversationManager(max_turns=3)\n",
    "            \n",
    "            # Create conversation\n",
    "            conv_id = conversation_manager.create_conversation(\"Test seed\")\n",
    "            \n",
    "            # Turn 1: Initial prompt\n",
    "            turn1_msg = [{\"role\": \"user\", \"content\": \"Tell me about AI safety\"}]\n",
    "            result1 = self.runner.generate_chat(turn1_msg)\n",
    "            conversation_manager.add_turn(conv_id, turn1_msg[0][\"content\"], result1[\"gen_text\"])\n",
    "            \n",
    "            # Turn 2: Follow-up based on response\n",
    "            turn2_msg = [\n",
    "                {\"role\": \"user\", \"content\": \"Tell me about AI safety\"},\n",
    "                {\"role\": \"assistant\", \"content\": result1[\"gen_text\"]},\n",
    "                {\"role\": \"user\", \"content\": \"Can you elaborate on potential risks?\"}\n",
    "            ]\n",
    "            result2 = self.runner.generate_chat(turn2_msg)\n",
    "            conversation_manager.add_turn(conv_id, turn2_msg[-1][\"content\"], result2[\"gen_text\"])\n",
    "            \n",
    "            if len(conversation_manager.conversations[conv_id][\"turns\"]) == 2:\n",
    "                self.log_test(\"8.1 Basic 2-turn conversation\", True, \n",
    "                            f\"Successfully created conversation with 2 turns\")\n",
    "            else:\n",
    "                self.log_test(\"8.1 Basic 2-turn conversation\", False, \n",
    "                            f\"Expected 2 turns, got {len(conversation_manager.conversations[conv_id]['turns'])}\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_test(\"8.1 Basic 2-turn conversation\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 8.2: Extended 3-turn conversation  \n",
    "        try:\n",
    "            conv_id2 = conversation_manager.create_conversation(\"Extended test\")\n",
    "            responses = []\n",
    "            \n",
    "            # Simulate 3 turns\n",
    "            for turn_num in range(1, 4):\n",
    "                if turn_num == 1:\n",
    "                    messages = [{\"role\": \"user\", \"content\": f\"Turn {turn_num}: What is machine learning?\"}]\n",
    "                else:\n",
    "                    # Build conversation history\n",
    "                    messages = []\n",
    "                    for prev_turn in conversation_manager.conversations[conv_id2][\"turns\"]:\n",
    "                        messages.append({\"role\": \"user\", \"content\": prev_turn[\"prompt\"]})\n",
    "                        messages.append({\"role\": \"assistant\", \"content\": prev_turn[\"response\"]})\n",
    "                    messages.append({\"role\": \"user\", \"content\": f\"Turn {turn_num}: Tell me more details\"})\n",
    "                \n",
    "                result = self.runner.generate_chat(messages)\n",
    "                responses.append(result)\n",
    "                conversation_manager.add_turn(conv_id2, messages[-1][\"content\"], result[\"gen_text\"])\n",
    "            \n",
    "            if len(conversation_manager.conversations[conv_id2][\"turns\"]) == 3:\n",
    "                self.log_test(\"8.2 Extended 3-turn conversation\", True,\n",
    "                            f\"Successfully created 3-turn conversation\")\n",
    "            else:\n",
    "                self.log_test(\"8.2 Extended 3-turn conversation\", False, \n",
    "                            f\"Expected 3 turns, got {len(conversation_manager.conversations[conv_id2]['turns'])}\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_test(\"8.2 Extended 3-turn conversation\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 8.3: Conversation context preservation\n",
    "        try:\n",
    "            # Create conversation with specific context that should be preserved\n",
    "            conv_id3 = conversation_manager.create_conversation(\"Context test\")\n",
    "            \n",
    "            # Turn 1: Establish context\n",
    "            context_msg = [{\"role\": \"user\", \"content\": \"My name is Alice and I work in cybersecurity\"}]\n",
    "            result1 = self.runner.generate_chat(context_msg)\n",
    "            conversation_manager.add_turn(conv_id3, context_msg[0][\"content\"], result1[\"gen_text\"])\n",
    "            \n",
    "            # Turn 2: Reference previous context\n",
    "            followup_msg = [\n",
    "                {\"role\": \"user\", \"content\": \"My name is Alice and I work in cybersecurity\"},\n",
    "                {\"role\": \"assistant\", \"content\": result1[\"gen_text\"]},\n",
    "                {\"role\": \"user\", \"content\": \"What security advice would you give someone in my field?\"}\n",
    "            ]\n",
    "            result2 = self.runner.generate_chat(followup_msg)\n",
    "            \n",
    "            # Check if context appears to be maintained (basic heuristic)\n",
    "            context_preserved = (\n",
    "                len(result2[\"gen_text\"]) > 20 and  # Response has substance\n",
    "                \"gen_text\" in result2 and  # Valid response structure\n",
    "                len(followup_msg) == 3  # Conversation history properly built\n",
    "            )\n",
    "            \n",
    "            if context_preserved:\n",
    "                self.log_test(\"8.3 Conversation context preservation\", True,\n",
    "                            f\"Context appears preserved in {len(result2['gen_text'])} char response\")\n",
    "            else:\n",
    "                self.log_test(\"8.3 Conversation context preservation\", False, \n",
    "                            \"Context preservation validation failed\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_test(\"8.3 Conversation context preservation\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        # Test 8.4: ConversationManager integration\n",
    "        try:\n",
    "            # Test conversation manager functionality\n",
    "            total_conversations = len(conversation_manager.conversations)\n",
    "            \n",
    "            # Verify all test conversations were created\n",
    "            expected_conversations = 3  # conv_id, conv_id2, conv_id3\n",
    "            \n",
    "            if total_conversations >= expected_conversations:\n",
    "                self.log_test(\"8.4 ConversationManager integration\", True,\n",
    "                            f\"Created {total_conversations} conversations successfully\")\n",
    "            else:\n",
    "                self.log_test(\"8.4 ConversationManager integration\", False,\n",
    "                            f\"Expected ‚â•{expected_conversations} conversations, got {total_conversations}\")\n",
    "                all_passed = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.log_test(\"8.4 ConversationManager integration\", False, f\"Error: {type(e).__name__}: {e}\")\n",
    "            all_passed = False\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "    def run_all_tests(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run the complete test suite\"\"\"\n",
    "        print(\"üß™ STARTING COMPREHENSIVE HUGGINGFACE RUNNER TEST SUITE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üéØ Target Model: {self.cfg.model.model_name}\")\n",
    "        print(f\"üîß Backend: {self.cfg.model.backend}\")\n",
    "        print(f\"üíæ Device: {self.cfg.model.device}\")\n",
    "        print(f\"üìä Max Tokens: {self.cfg.model.max_new_tokens}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run all test groups\n",
    "        test_groups = [\n",
    "            (\"Input Validation\", self.test_input_validation),\n",
    "            (\"Message Formats\", self.test_message_formats),\n",
    "            (\"Token Budget\", self.test_token_budget_handling),\n",
    "            (\"Generation Parameters\", self.test_generation_parameters),\n",
    "            (\"Chat Templates\", self.test_chat_template_handling),\n",
    "            (\"Error Handling\", self.test_error_handling_robustness),\n",
    "            (\"Performance\", self.test_performance_benchmarks),\n",
    "            (\"Multi-turn Conversations\", self.test_multi_turn_conversations)\n",
    "        ]\n",
    "        \n",
    "        group_results = {}\n",
    "        for group_name, test_func in test_groups:\n",
    "            try:\n",
    "                group_results[group_name] = test_func()\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå CRITICAL ERROR in {group_name}: {type(e).__name__}: {e}\")\n",
    "                group_results[group_name] = False\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Generate summary\n",
    "        print(f\"\\nüèÅ TEST SUITE COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"‚è±Ô∏è  Total runtime: {total_time:.2f} seconds\")\n",
    "        print(f\"üìä Tests run: {len(self.test_results)}\")\n",
    "        \n",
    "        passed_count = sum(1 for result in self.test_results if result['passed'])\n",
    "        failed_count = len(self.test_results) - passed_count\n",
    "        \n",
    "        print(f\"‚úÖ Passed: {passed_count}\")\n",
    "        print(f\"‚ùå Failed: {failed_count}\")\n",
    "        \n",
    "        if failed_count == 0:\n",
    "            print(\"üéâ ALL TESTS PASSED - HuggingFaceRunner is working correctly!\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {failed_count} test(s) failed. See details above.\")\n",
    "            print(f\"üîß Failed tests: {', '.join(self.failed_tests)}\")\n",
    "        \n",
    "        success_rate = (passed_count / len(self.test_results)) * 100 if self.test_results else 0\n",
    "        print(f\"üìà Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"total_tests\": len(self.test_results),\n",
    "            \"passed\": passed_count,\n",
    "            \"failed\": failed_count,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"runtime\": total_time,\n",
    "            \"failed_tests\": self.failed_tests,\n",
    "            \"group_results\": group_results,\n",
    "            \"detailed_results\": self.test_results\n",
    "        }\n",
    "\n",
    "# Execute the comprehensive test suite\n",
    "print(\"üöÄ Initializing HuggingFaceRunner Test Suite...\")\n",
    "\n",
    "if 'runner' in globals() and runner is not None:\n",
    "    tester = HuggingFaceRunnerTester(runner, cfg)\n",
    "    test_summary = tester.run_all_tests()\n",
    "    \n",
    "    # Store test results for future reference\n",
    "    globals()['hf_runner_test_results'] = test_summary\n",
    "    \n",
    "    if test_summary['success_rate'] >= 90:\n",
    "        print(\"\\nüèÜ EXCELLENT: HuggingFaceRunner passes professional testing standards!\")\n",
    "    elif test_summary['success_rate'] >= 75:\n",
    "        print(\"\\nüëç GOOD: HuggingFaceRunner is functional with minor issues.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  ATTENTION NEEDED: Multiple test failures require investigation.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Runner not initialized. Please run the framework initialization cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red_team_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
