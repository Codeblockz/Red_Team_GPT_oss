{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip -q install -U transformers torch accelerate safetensors ipywidgets openai-harmony kernels\n",
    "\n",
    "# Jupyter widgets are usually auto-enabled nowadays. If you don't see them,\n",
    "# save & restart the kernel after this cell.\n",
    "import ipywidgets as widgets  # sanity import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: openai/gpt-oss-20b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e22d19bf05e4a128808997e256321a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98917ca08850472e95c6fe788175fce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Harmony helpers for parsing channels/roles from generated token IDs\n",
    "from openai_harmony import (\n",
    "    load_harmony_encoding,\n",
    "    HarmonyEncodingName,\n",
    "    Role,\n",
    ")\n",
    "\n",
    "# UI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# --- Config (override via environment vars if you want) ---\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\", \"openai/gpt-oss-20b\")\n",
    "REASONING_EFFORT = os.getenv(\"REASONING_EFFORT\", \"high\")   # 'low' | 'medium' | 'high'\n",
    "MAX_NEW_TOKENS = int(os.getenv(\"MAX_NEW_TOKENS\", \"1024\"))\n",
    "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n",
    "TOP_P = float(os.getenv(\"TOP_P\", \"0.95\"))\n",
    "\n",
    "print(\"Using model:\", MODEL_ID)\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Loaded on device:\", device)\n",
    "\n",
    "# Load Harmony encoding/parser\n",
    "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_system_message() -> Dict[str, str]:\n",
    "    \"\"\"Harmony-style system message (adds date + desired reasoning effort).\"\"\"\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    sys_text = f\"\"\"You are ChatGPT, a large language model trained by OpenAI.\n",
    "Knowledge cutoff: 2024-06\n",
    "Current date: {today}\n",
    "\n",
    "Reasoning: {REASONING_EFFORT}\n",
    "\n",
    "# Valid channels: analysis, commentary, final. Channel must be included for every message.\"\"\"\n",
    "    return {\"role\": \"system\", \"content\": sys_text}\n",
    "\n",
    "def apply_chat_template(messages: List[Dict[str, str]]):\n",
    "    \"\"\"Use Transformers' chat template (applies Harmony formatting automatically).\"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Handle case where apply_chat_template returns a tensor directly\n",
    "    if isinstance(inputs, torch.Tensor):\n",
    "        return {\"input_ids\": inputs.to(device)}\n",
    "    else:\n",
    "        return {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "def parse_harmony_from_new_tokens(new_token_ids: List[int]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parse analysis (thinking) and final strings from generated tokens using Harmony.\n",
    "    \"\"\"\n",
    "    msgs = encoding.parse_messages_from_completion_tokens(new_token_ids, Role.ASSISTANT)\n",
    "    analysis_chunks = []\n",
    "    final_text = \"\"\n",
    "    for m in msgs:\n",
    "        ch = getattr(m, \"channel\", \"\") or \"\"\n",
    "        text = getattr(m, \"content\", \"\") or \"\"\n",
    "        \n",
    "        # Ensure text is always a string\n",
    "        if isinstance(text, list):\n",
    "            text = \"\".join(str(t) for t in text)\n",
    "        elif not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if ch == \"analysis\":\n",
    "            analysis_chunks.append(text)\n",
    "        elif ch == \"final\":\n",
    "            final_text += text\n",
    "    return (\"\\n\".join(analysis_chunks).strip(), final_text.strip())\n",
    "\n",
    "def generate_one_turn(messages: List[Dict[str, str]]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate one assistant turn; returns dict with 'analysis' and 'final'.\n",
    "    Only append the 'final' back to history to maintain clean user-visible chat.\n",
    "    \"\"\"\n",
    "    inputs = apply_chat_template(messages)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # Keep only the newly generated tokens\n",
    "    new_ids = out[0].tolist()[inputs[\"input_ids\"].shape[1]:]\n",
    "    analysis, final = parse_harmony_from_new_tokens(new_ids)\n",
    "    return {\"analysis\": analysis, \"final\": final, \"new_token_ids\": new_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat primed with system + developer messages.\n"
     ]
    }
   ],
   "source": [
    "# Initialize chat history\n",
    "chat: list[dict] = [build_system_message()]\n",
    "\n",
    "# Optional \"developer\" message per Harmony guidance\n",
    "chat.append({\n",
    "    \"role\": \"developer\",\n",
    "    \"content\": (\n",
    "        \"You are a helpful assistant. Keep answers crisp unless asked otherwise. \"\n",
    "        \"Use Markdown formatting when appropriate.\"\n",
    "    ),\n",
    "})\n",
    "\n",
    "print(\"Chat primed with system + developer messages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_cli():\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "    local_chat = chat.copy()\n",
    "    while True:\n",
    "        user_text = input(\"You: \").strip()\n",
    "        if not user_text or user_text.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        local_chat.append({\"role\": \"user\", \"content\": user_text})\n",
    "        turn = generate_one_turn(local_chat)\n",
    "\n",
    "        print(\"\\n—— Thinking ——————————————\")\n",
    "        print(turn[\"analysis\"] if turn[\"analysis\"] else \"(no analysis emitted)\")\n",
    "        print(\"—— Final ————————————————\")\n",
    "        print(turn[\"final\"])\n",
    "        print(\"—————————————————————————\\n\")\n",
    "\n",
    "        # Append only the final answer back to history\n",
    "        local_chat.append({\"role\": \"assistant\", \"content\": turn[\"final\"]})\n",
    "\n",
    "# Uncomment to use the CLI:\n",
    "# chat_cli()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76519/2480170364.py:67: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  inp.on_submit(on_send)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55ec3752eac44358e3c3dc548da5bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Checkbox(value=True, description='Show thinking (analysis)'),)), Output(), HBox(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_thinking = widgets.Checkbox(value=True, description=\"Show thinking (analysis)\")\n",
    "inp = widgets.Text(placeholder=\"Ask me anything…\", description=\"You\")\n",
    "send_btn = widgets.Button(description=\"Send\", button_style=\"primary\")\n",
    "out = widgets.Output()\n",
    "\n",
    "header = widgets.HBox([show_thinking])\n",
    "composer = widgets.HBox([inp, send_btn])\n",
    "app = widgets.VBox([header, out, composer])\n",
    "\n",
    "def render_history(local_chat: list[dict], last_turn: dict | None):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        display(HTML(\"<h3>Conversation</h3>\"))\n",
    "        for msg in local_chat:\n",
    "            role = msg.get(\"role\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                display(HTML(f\"<div><b>🧑 You:</b> {content}</div><hr/>\"))\n",
    "            elif role == \"assistant\":\n",
    "                display(HTML(f\"<div><b>🤖 Assistant:</b><br/>{content}</div><hr/>\"))\n",
    "            elif role in {\"system\", \"developer\"}:\n",
    "                display(HTML(\n",
    "                    f\"\"\"\n",
    "<details>\n",
    "  <summary><b>⚙️ {role.title()} message</b> (click to expand)</summary>\n",
    "  <pre style='white-space:pre-wrap'>{content}</pre>\n",
    "</details><hr/>\n",
    "\"\"\"))\n",
    "        if last_turn:\n",
    "            if show_thinking.value:\n",
    "                thinking = last_turn.get(\"analysis\", \"\").strip()\n",
    "                if thinking:\n",
    "                    display(HTML(\n",
    "                        f\"\"\"\n",
    "<details open>\n",
    "  <summary><b>🧠 Thinking (analysis)</b></summary>\n",
    "  <pre style='white-space:pre-wrap'>{thinking}</pre>\n",
    "</details>\n",
    "\"\"\"))\n",
    "            final = last_turn.get(\"final\", \"\").strip()\n",
    "            if final:\n",
    "                display(HTML(f\"<div><b>💬 Final:</b><br/>{final}</div>\"))\n",
    "        display(HTML(\"<hr/>\"))\n",
    "\n",
    "local_chat = chat.copy()\n",
    "last_turn = None\n",
    "render_history(local_chat, last_turn)\n",
    "\n",
    "def on_send(_):\n",
    "    global last_turn\n",
    "    text = inp.value.strip()\n",
    "    if not text:\n",
    "        return\n",
    "    inp.value = \"\"\n",
    "    local_chat.append({\"role\": \"user\", \"content\": text})\n",
    "    render_history(local_chat, None)\n",
    "\n",
    "    # Generate assistant turn\n",
    "    turn = generate_one_turn(local_chat)\n",
    "    last_turn = turn\n",
    "\n",
    "    # Append only final text back to history to preserve multi-turn continuity\n",
    "    local_chat.append({\"role\": \"assistant\", \"content\": turn[\"final\"]})\n",
    "    render_history(local_chat, turn)\n",
    "\n",
    "send_btn.on_click(on_send)\n",
    "inp.on_submit(on_send)\n",
    "\n",
    "display(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-24\\n\\nReasoning: high\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.'},\n",
       " {'role': 'developer',\n",
       "  'content': 'You are a helpful assistant. Keep answers crisp unless asked otherwise. Use Markdown formatting when appropriate.'},\n",
       " {'role': 'user', 'content': 'tell a joke'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"text='Why don’t scientists trust atoms?\\\\n\\\\nBecause they make up everything!'\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: String content\n",
      "Analysis: ['This is analysis text']\n",
      "Final: This is final text\n",
      "✓ Test 1 passed\n",
      "\n",
      "Test 2: List content\n",
      "Analysis: ['This is analysis']\n",
      "Final: This is final\n",
      "✓ Test 2 passed\n",
      "\n",
      "Test 3: Mixed content types\n",
      "Analysis: ['12345', 'Normal string']\n",
      "Final: Mixed content and more\n",
      "✓ Test 3 passed\n",
      "\n",
      "All tests passed! The fix should prevent TypeError.\n"
     ]
    }
   ],
   "source": [
    "# Test block to verify the parse_harmony_from_new_tokens fix\n",
    "def test_parse_harmony_fix():\n",
    "    \"\"\"Test the parse_harmony_from_new_tokens function with various data types.\"\"\"\n",
    "    \n",
    "    # Mock message objects to simulate different content types\n",
    "    class MockMessage:\n",
    "        def __init__(self, channel, content):\n",
    "            self.channel = channel\n",
    "            self.content = content\n",
    "    \n",
    "    # Test case 1: String content (normal case)\n",
    "    print(\"Test 1: String content\")\n",
    "    test_msgs_str = [\n",
    "        MockMessage(\"analysis\", \"This is analysis text\"),\n",
    "        MockMessage(\"final\", \"This is final text\")\n",
    "    ]\n",
    "    \n",
    "    # Manually test the logic\n",
    "    analysis_chunks = []\n",
    "    final_text = \"\"\n",
    "    for m in test_msgs_str:\n",
    "        ch = getattr(m, \"channel\", \"\") or \"\"\n",
    "        text = getattr(m, \"content\", \"\") or \"\"\n",
    "        \n",
    "        # Apply the fix\n",
    "        if isinstance(text, list):\n",
    "            text = \"\".join(str(t) for t in text)\n",
    "        elif not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if ch == \"analysis\":\n",
    "            analysis_chunks.append(text)\n",
    "        elif ch == \"final\":\n",
    "            final_text += text\n",
    "    \n",
    "    print(f\"Analysis: {analysis_chunks}\")\n",
    "    print(f\"Final: {final_text}\")\n",
    "    print(\"✓ Test 1 passed\\n\")\n",
    "    \n",
    "    # Test case 2: List content (problematic case)\n",
    "    print(\"Test 2: List content\")\n",
    "    test_msgs_list = [\n",
    "        MockMessage(\"analysis\", [\"This\", \" is\", \" analysis\"]),\n",
    "        MockMessage(\"final\", [\"This\", \" is\", \" final\"])\n",
    "    ]\n",
    "    \n",
    "    analysis_chunks = []\n",
    "    final_text = \"\"\n",
    "    for m in test_msgs_list:\n",
    "        ch = getattr(m, \"channel\", \"\") or \"\"\n",
    "        text = getattr(m, \"content\", \"\") or \"\"\n",
    "        \n",
    "        # Apply the fix\n",
    "        if isinstance(text, list):\n",
    "            text = \"\".join(str(t) for t in text)\n",
    "        elif not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if ch == \"analysis\":\n",
    "            analysis_chunks.append(text)\n",
    "        elif ch == \"final\":\n",
    "            final_text += text\n",
    "    \n",
    "    print(f\"Analysis: {analysis_chunks}\")\n",
    "    print(f\"Final: {final_text}\")\n",
    "    print(\"✓ Test 2 passed\\n\")\n",
    "    \n",
    "    # Test case 3: Mixed content types\n",
    "    print(\"Test 3: Mixed content types\")\n",
    "    test_msgs_mixed = [\n",
    "        MockMessage(\"analysis\", 12345),  # Integer\n",
    "        MockMessage(\"final\", [\"Mixed\", \" content\"]),  # List\n",
    "        MockMessage(\"analysis\", \"Normal string\"),  # String\n",
    "        MockMessage(\"final\", \" and more\")  # String\n",
    "    ]\n",
    "    \n",
    "    analysis_chunks = []\n",
    "    final_text = \"\"\n",
    "    for m in test_msgs_mixed:\n",
    "        ch = getattr(m, \"channel\", \"\") or \"\"\n",
    "        text = getattr(m, \"content\", \"\") or \"\"\n",
    "        \n",
    "        # Apply the fix\n",
    "        if isinstance(text, list):\n",
    "            text = \"\".join(str(t) for t in text)\n",
    "        elif not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        if ch == \"analysis\":\n",
    "            analysis_chunks.append(text)\n",
    "        elif ch == \"final\":\n",
    "            final_text += text\n",
    "    \n",
    "    print(f\"Analysis: {analysis_chunks}\")\n",
    "    print(f\"Final: {final_text}\")\n",
    "    print(\"✓ Test 3 passed\\n\")\n",
    "    \n",
    "    print(\"All tests passed! The fix should prevent TypeError.\")\n",
    "\n",
    "# Run the test\n",
    "test_parse_harmony_fix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "red_team_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
